{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aliyagangji/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aliyagangji/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aliyagangji/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import sklearn.neighbors\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction import text\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#lemmatization libraries \n",
    "import nltk.stem\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# punctuation + stop words libraries:\n",
    "from nltk.corpus import stopwords\n",
    "import re, string, timeit\n",
    "\n",
    "#stemming \n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1(a) Parsing the txt files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1(a): Parsing Yelp, IMDB, and Amazon database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = pd.read_csv(\"yelp_labelled.txt\", sep=\"\\t\", header=None, names=[\"sentence\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many values are there? How many positive and negative emotions (0/1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aliyagangji/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df_imdb = pd.read_csv(\"imdb_labelled.txt\", sep=\" \\t\", header=None, names=[\"sentence\", \"label\"])\n",
    "#Notice: IMDB required another space in order to parse the 1000 lines because it was badly formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many values are there? How many positive and negative emotions (0/1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imdb.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Amazon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon = pd.read_csv(\"amazon_cells_labelled.txt\", sep=\"\\t\", header=None, names=[\"sentence\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many values are there? How many positive and negative emotions (0/1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results - Amazon, IMDB and Yelp are balanced (50% negative 50% positive)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(b): Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick your preprocessing strategy. Since these sentences are online reviews, they may contain significant amounts of noise and garbage. You may or may not want to do one or all of\n",
    "the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Lowercase all of the words.\n",
    "\n",
    "• Lemmatization of all the words (i.e., convert every word to its root so that all of “running,”\n",
    "“run,” and “runs” are converted to “run” and and all of “good,” “well,” “better,” and “best”\n",
    "are converted to “good”; this is easily done using nltk.stem).\n",
    "\n",
    "• Strip punctuation.\n",
    "\n",
    "• Strip the stop words, e.g., “the”, “and”, “or”.\n",
    "\n",
    "• Something else? Tell us about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase all databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[\"sentence\"] = df_yelp[\"sentence\"].str.lower()\n",
    "df_imdb[\"sentence\"] = df_imdb[\"sentence\"].str.lower()\n",
    "df_amazon[\"sentence\"] = df_amazon[\"sentence\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strip the stop words, e.g., “the”, “and”, “or”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop.remove(\"not\")\n",
    "stop.remove(\"nor\")\n",
    "# stop = text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['sentence'] = df_yelp['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df_imdb['sentence'] = df_imdb['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df_amazon['sentence'] = df_amazon['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuation Strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that removes punctuation:\n",
    "def remove_punctuation(sentence):\n",
    "    sentence = re.sub(r'[^\\w\\s+]','',sentence)\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[\"sentence\"] = df_yelp['sentence'].apply(lambda x: remove_punctuation(x))\n",
    "df_imdb[\"sentence\"] = df_imdb['sentence'].apply(lambda x: remove_punctuation(x))\n",
    "df_amazon[\"sentence\"] = df_amazon['sentence'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = nltk.tokenize.WhitespaceTokenizer() \n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    list2 = word_tokenizer.tokenize(text)\n",
    "    lemmatized_sentence = ' '.join([lemmatizer.lemmatize(words) for words in list2])\n",
    "    return(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['sentence'] = df_yelp['sentence'].apply(lambda x: lemmatize_text(x))\n",
    "df_imdb['sentence'] = df_imdb['sentence'].apply(lambda x: lemmatize_text(x))\n",
    "df_amazon['sentence'] = df_amazon['sentence'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2(C): Split training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each file use the first 400 instances for each label as the training set and the remaining 100 instances as testing set.\n",
    "\n",
    "In total, there are 2400 reviews for training and 600 reviews for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yelp - Divide to positive and negative data set and extract from there the train / test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into positive / negative datasets:\n",
    "df_yelp_pos = df_yelp[df_yelp['label'] == 1].reset_index(drop=True)   # positive emotion = 1\n",
    "df_yelp_neg = df_yelp[df_yelp['label'] == 0].reset_index(drop=True)   # Negative emotion = 0\n",
    "\n",
    "# For the positive dataset - divide into train / test datasets:\n",
    "df_yelp_train_pos = df_yelp_pos[:400]\n",
    "df_yelp_test_pos = df_yelp_pos[400:500]\n",
    "\n",
    "# For the negative dataset - divide into train / test datasets:\n",
    "df_yelp_train_neg = df_yelp_neg[:400]\n",
    "df_yelp_test_neg = df_yelp_neg[400:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB - Divide to positive and negative data set and extract from there the train / test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into positive / negative datasets:\n",
    "df_imdb_pos = df_imdb[df_imdb['label'] == 1].reset_index(drop=True)   # positive emotion = 1\n",
    "df_imdb_neg = df_imdb[df_imdb['label'] == 0].reset_index(drop=True)   # Negative emotion = 0\n",
    "\n",
    "# For positive - train / test datasets:\n",
    "df_imdb_train_pos = df_imdb_pos[:400]\n",
    "df_imdb_test_pos = df_imdb_pos[400:500]\n",
    "\n",
    "# For the negative dataset - divide into train / test datasets:\n",
    "df_imdb_train_neg = df_imdb_neg[:400]\n",
    "df_imdb_test_neg = df_imdb_neg[400:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon - Divide to positive and negative data set and extract from there the train / test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into positive / negative datasets:\n",
    "df_amazon_pos = df_amazon[df_amazon['label'] == 1].reset_index(drop=True)   # positive emotion = 1\n",
    "df_amazon_neg = df_amazon[df_amazon['label'] == 0].reset_index(drop=True)   # Negative emotion = 0\n",
    "\n",
    "# For positive - train / test datasets:\n",
    "df_amazon_train_pos = df_amazon_pos[:400]\n",
    "df_amazon_test_pos = df_amazon_pos[400:500]\n",
    "\n",
    "# For the negative dataset - divide into train / test datasets:\n",
    "df_amazon_train_neg = df_amazon_neg[:400]\n",
    "df_amazon_test_neg = df_amazon_neg[400:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat - train_df & test_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.concat([df_amazon_train_pos, df_amazon_train_neg, df_yelp_train_pos, df_yelp_train_neg,\n",
    "                                   df_imdb_train_pos, df_imdb_train_neg], ignore_index=True)\n",
    "\n",
    "#self check\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.concat([df_amazon_test_pos, df_amazon_test_neg, df_yelp_test_pos, df_yelp_test_neg,\n",
    "                                   df_yelp_test_pos, df_yelp_test_neg], ignore_index=True)\n",
    "\n",
    "#self check\n",
    "len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2(h)\n",
    "df_train_ngram = df_train.copy()\n",
    "df_test_ngram = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words model.\n",
    "\n",
    "Extract features and then represent each review using bag of words model, i.e., every word in the review becomes its own element in a feature vector. \n",
    "In order to do this, first, make one pass through all the reviews in the training set (Explain why we can’t\n",
    "use testing set at this point) and build a dictionary of unique words. \n",
    "Then, make another pass through the review in both the training set and testing set and count up the occurrences of\n",
    "each word in your dictionary. \n",
    "The i-th element of a review’s feature vector is the number of occurrences of the ith dictionary word in the review. \n",
    "Implement the bag of words model and report feature vectors of any two reviews in the training set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dictionary with all unique words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with all unique words and their count:\n",
    "word_freq = dict()\n",
    "for row in df_train['sentence']:\n",
    "    for word in row.split():\n",
    "        if word not in word_freq:\n",
    "            word_freq[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count values\n",
    "for row in df_train['sentence']:\n",
    "    for word in row.split():\n",
    "        word_freq[word] = word_freq[word] + 1\n",
    "for row in df_test['sentence']:\n",
    "    for word in row.split():\n",
    "        if word in word_freq:\n",
    "            word_freq[word] = word_freq[word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrow_dict = word_freq.copy()\n",
    "# for k, v in word_freq.items():\n",
    "#      if v < 3:\n",
    "#             del narrow_dict[k]\n",
    "# len(narrow_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vector_func(sentence):\n",
    "    feature_vector = {x:0 for x in word_freq} # Put 0-s instead of counting every unique word\n",
    "    word_list = sentence.split()\n",
    "    for word in feature_vector:\n",
    "        if word in word_list:\n",
    "            feature_vector[word] = feature_vector[word] + 1\n",
    "        else:\n",
    "            feature_vector[word] = feature_vector[word]\n",
    "    return(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the function to all rows in df_train to get all feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['feature_vector_dict'] = df_train['sentence'].apply(lambda x: feature_vector_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not wanted</td>\n",
       "      <td>0</td>\n",
       "      <td>{'not': 1, 'wanted': 1, 'pulled': 0, 'car': 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pulled car waited another 15 minute acknowledged</td>\n",
       "      <td>0</td>\n",
       "      <td>{'not': 0, 'wanted': 0, 'pulled': 1, 'car': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>today first taste buldogis gourmet hot dog tel...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>place not worth time let alone vega</td>\n",
       "      <td>0</td>\n",
       "      <td>{'not': 1, 'wanted': 0, 'pulled': 0, 'car': 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>easy use</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0                                         not wanted      0   \n",
       "1   pulled car waited another 15 minute acknowledged      0   \n",
       "2  today first taste buldogis gourmet hot dog tel...      1   \n",
       "3                place not worth time let alone vega      0   \n",
       "4                                           easy use      1   \n",
       "\n",
       "                                 feature_vector_dict  \n",
       "0  {'not': 1, 'wanted': 1, 'pulled': 0, 'car': 0,...  \n",
       "1  {'not': 0, 'wanted': 0, 'pulled': 1, 'car': 1,...  \n",
       "2  {'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...  \n",
       "3  {'not': 1, 'wanted': 0, 'pulled': 0, 'car': 0,...  \n",
       "4  {'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the function to all rows in the same manner for df_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['feature_vector_dict'] = df_test['sentence'].apply(lambda x: feature_vector_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>level 5 spicy perfect spice overwhelm soup</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good quality bargain bought bought cheapy big ...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love ownerchef one authentic japanese cool dude</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excellent</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pairing iphone could not happier far</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not': 1, 'wanted': 0, 'pulled': 0, 'car': 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0         level 5 spicy perfect spice overwhelm soup      1   \n",
       "1  good quality bargain bought bought cheapy big ...      1   \n",
       "2    love ownerchef one authentic japanese cool dude      1   \n",
       "3                                          excellent      1   \n",
       "4               pairing iphone could not happier far      1   \n",
       "\n",
       "                                 feature_vector_dict  \n",
       "0  {'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...  \n",
       "1  {'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...  \n",
       "2  {'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...  \n",
       "3  {'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...  \n",
       "4  {'not': 1, 'wanted': 0, 'pulled': 0, 'car': 0,...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare any two feature vectors of any two reviews in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'not': 1, 'wanted': 1, 'pulled': 0, 'car': 0,...\n",
       "1    {'not': 0, 'wanted': 0, 'pulled': 1, 'car': 1,...\n",
       "Name: feature_vector_dict, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['feature_vector_dict'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(e) - Pick your postprocessing strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We chose to use the 4th method - standardize the data by subtracting the mean and dividing by the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. \n",
    "\n",
    "However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. \n",
    "\n",
    "So, even if you have outliers in your data, they will not be affected by standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that converts the dict \"feature_vector\" into an array:\n",
    "def dict_to_array(vector):\n",
    "    data = list(vector.values())\n",
    "    an_array = np.array(data)\n",
    "    return(an_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = list(df_train['feature_vector'][1].values())\n",
    "# an_array = np.array(data)\n",
    "# print(an_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['feature_vector_array'] = df_train['feature_vector_dict'].apply(lambda x: dict_to_array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #self check - ignore\n",
    "# arr_2 = df_train['feature_vector_array'][2]\n",
    "# for i in arr_2:\n",
    "#     if i == 1:\n",
    "#         print(i)\n",
    "\n",
    "# df_train['sentence'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that gets \"df_train['array_vector'][i]\", and returns the standardized array:\n",
    "\n",
    "import itertools\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def standard(row):\n",
    "    standardized_array = scaler.fit_transform(np.array(row).reshape(-1,1))\n",
    "    return(standardized_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['standard_array'] = df_train['feature_vector_array'].apply(lambda x: standard(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.64435141e+01],\n",
       "       [ 4.64435141e+01],\n",
       "       [-2.15315318e-02],\n",
       "       ...,\n",
       "       [-2.15315318e-02],\n",
       "       [-2.15315318e-02],\n",
       "       [-2.15315318e-02]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#self check - ignore\n",
    "df_train['standard_array'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict</th>\n",
       "      <th>feature_vector_array</th>\n",
       "      <th>standard_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not wanted</td>\n",
       "      <td>0</td>\n",
       "      <td>{'not': 1, 'wanted': 1, 'pulled': 0, 'car': 0,...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[46.44351407893248], [46.44351407893248], [-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pulled car waited another 15 minute acknowledged</td>\n",
       "      <td>0</td>\n",
       "      <td>{'not': 0, 'wanted': 0, 'pulled': 1, 'car': 1,...</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.04030517145647683], [-0.04030517145647683...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>today first taste buldogis gourmet hot dog tel...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[[-0.05054867366041315], [-0.05054867366041315...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>place not worth time let alone vega</td>\n",
       "      <td>0</td>\n",
       "      <td>{'not': 1, 'wanted': 0, 'pulled': 0, 'car': 0,...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[24.810711972279808], [-0.04030517145647683],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>easy use</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.021531531793663645], [-0.0215315317936636...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0                                         not wanted      0   \n",
       "1   pulled car waited another 15 minute acknowledged      0   \n",
       "2  today first taste buldogis gourmet hot dog tel...      1   \n",
       "3                place not worth time let alone vega      0   \n",
       "4                                           easy use      1   \n",
       "\n",
       "                                 feature_vector_dict  \\\n",
       "0  {'not': 1, 'wanted': 1, 'pulled': 0, 'car': 0,...   \n",
       "1  {'not': 0, 'wanted': 0, 'pulled': 1, 'car': 1,...   \n",
       "2  {'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...   \n",
       "3  {'not': 1, 'wanted': 0, 'pulled': 0, 'car': 0,...   \n",
       "4  {'not': 0, 'wanted': 0, 'pulled': 0, 'car': 0,...   \n",
       "\n",
       "                                feature_vector_array  \\\n",
       "0  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                      standard_array  \n",
       "0  [[46.44351407893248], [46.44351407893248], [-0...  \n",
       "1  [[-0.04030517145647683], [-0.04030517145647683...  \n",
       "2  [[-0.05054867366041315], [-0.05054867366041315...  \n",
       "3  [[24.810711972279808], [-0.04030517145647683],...  \n",
       "4  [[-0.021531531793663645], [-0.0215315317936636...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['feature_vector_array'] = df_test['feature_vector_dict'].apply(lambda x: dict_to_array(x))\n",
    "df_test['standard_array'] = df_test['feature_vector_array'].apply(lambda x: standard(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in range(0, len(df_test['feature_vector_array'])):\n",
    "#     for i in df_test['feature_vector_array'][row]:\n",
    "#         if i==1:\n",
    "#             print(row, i, df_test['sentence'][row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(f) - Sentiment Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a naive Bayes model on the training set and test on the testing set. \n",
    "\n",
    "Report the classification accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the data comfortable to use:\n",
    "def array_of_arrays(row):\n",
    "    list_standard = []\n",
    "    for i in range(0, len(row)):\n",
    "        list_standard.append((row[i][0]))\n",
    "    return (np.array(list_standard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['standard_array_clean'] = df_train['standard_array'].apply(lambda x: array_of_arrays(x))\n",
    "df_test['standard_array_clean'] = df_test['standard_array'].apply(lambda x: array_of_arrays(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = df_train[\"standard_array_clean\"]\n",
    "test_X = df_test[\"standard_array_clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.DataFrame(train_X.to_list()).to_numpy()\n",
    "test_X = pd.DataFrame(test_X.to_list()).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Check that there is no \"Nan\" values:\n",
    "\n",
    "for array in test_X:\n",
    "    array_sum = np.sum(array)\n",
    "    array_has_nan = np.isnan(array_sum)\n",
    "print(array_has_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = df_train['label']\n",
    "test_Y = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = train_X.shape[0] #number of rows\n",
    "d = train_X.shape[1] #number of unique words = features in feature vector\n",
    "K = 2 #number of classes - label 1 or label 0\n",
    "\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    X_k = train_X[train_Y == k]\n",
    "    phis[k] = X_k.shape[0] / float(n)\n",
    "    psis[k] = np.mean(X_k, axis=0) #build a function with the mean for label 0 and mean for label 0 as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(psis.shape, phis.shape, train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement this in numpy\n",
    "def nb_predictions(x, psis, phis):\n",
    "    \"\"\"This returns class assignments and scores under the NB model.\n",
    "    \n",
    "    We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n",
    "    \"\"\"\n",
    "    # adjust shapes\n",
    "    n , d = x.shape\n",
    "    x = np.reshape(x, (1,n,d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14) #understand\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape(K,1)\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes on the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train, logpyx = nb_predictions(train_X, psis, phis)\n",
    "# len(predicted_train), len(logpyx), predicted_train.shape, logpyx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8779166666666667"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_train == train_Y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test, logpyx_test = nb_predictions(test_X, psis, phis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7533333333333333"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_test == test_Y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = test_Y\n",
    "predictions = predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Predicted', ylabel='Actual'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEGCAYAAAB4lx7eAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAbTklEQVR4nO3deZhV1Znv8e+vClExOAACMhhRMIjGKQbTcUBNVNAYoJOrGGOevkIqaCsxneTqc5OOGexuExNtk2gQjbGvraKJoUMURSUqjkkRxQEERTRazKCIAxHq1Hv/OIfiVHGqahecaZe/j89+OHvvtfZex6d4a/HutddSRGBmZulVU+kGmJnZjnEgNzNLOQdyM7OUcyA3M0s5B3Izs5TrVukGtGXz2qUeTmPb2HXAcZVuglWhxk3LtKPX6EzM2anP/jt8v2Jyj9zMLOWqtkduZlZWTZlKt2C7OZCbmQFkGivdgu3m1IqZGRDRlHjriKTRkhZLWiLp0nbKfVJSRtIXO1s3nwO5mRlAU1PyrR2SaoFrgTHACOBsSSPaKPdjYHZn67bmQG5mBhBNybf2jQSWRMTSiNgETAfGFih3EXAXsHo76rbgQG5mBtmHnQk3SXWS5uVtdXlXGgi8kbffkDvWTNJAYDwwtVUrOqxbiB92mplBkp721qIR04BpbZwuNMa89Rj1/wQuiYiM1KJ4krrbcCA3MwOieKNWGoDBefuDgOWtyhwFTM8F8T7AaZIaE9bdhgO5mRl0+BCzE+qBYZKGAMuACcCX8gtExJAtnyXdDNwdEf8jqVtHdQtxIDczg06lVtq9TESjpAvJjkapBW6KiAWSJufOt86Ld1i3o3uqWlcI8lwrVojnWrFCijHXygeLHkkcc3YePqqq5lpxj9zMDIrWI68EB3IzM0j1K/oO5GZmUMyHnWXnQG5mBkR49kMzs3RzjtzMLOWcWjEzSzn3yM3MUi6zudIt2G4O5GZm4NSKmVnqObViZpZy7pGbmaWcA7mZWbqFH3aamaWcc+RmZinn1IqZWcq5R25mlnLukZuZpZx75GZmKdfohSXMzNLNPXIzs5RzjtzMLOXcIzczSzn3yM3MUs49cjOzlEvxqJWaSjfAzKwqRCTfOiBptKTFkpZIurTA+bGSnpM0X9I8ScfmnXtN0vNbziVpunvkZmZQtBy5pFrgWuBkoAGolzQzIhbmFZsDzIyIkHQocCcwPO/8iRGxNuk9HcjNzKCYDztHAksiYimApOnAWKA5kEfEu3nldwM67ua3w6kVMzPIPuxMuEmqy6VEtmx1eVcaCLyRt9+QO9aCpPGSFgH3AOfltwS4X9JfW123Te6Rm5kBZDKJi0bENGBaG6dVqEqBa8wAZkg6HvgR8NncqWMiYrmkvsADkhZFxNz22uMeuZkZZFMrSbf2NQCD8/YHAcvbKpwL0gdI6pPbX577czUwg2yqpl0O5GZmUMxAXg8MkzREUndgAjAzv4CkoZKU+3wk0B1YJ2k3ST1zx3cDTgFe6OiGTq2YmUHRXgiKiEZJFwKzgVrgpohYIGly7vxU4AvAVyRtBjYCZ+VGsPQjm26BbHy+LSLu6+ieDuRmZkA07dDAkZbXipgFzGp1bGre5x8DPy5QbylwWGfv50BuZgaea8XMLPU6MWql2jiQm5mBe+RmZqmX4kDu4YcV9thT8/jchEmMOfM8brzlzjbLPf/iYg497nTuf+jR5mPf/ferOP70CYz78uRyNNXK6NRTTmDBC3NZtPAx/s+3/7lgmauv+iGLFj7G0399gCMOP6T5+EUXTmT+M3N4dv6fmHLRpHI1Of2KOGlWuTmQV1Amk+Hyn13Lr372I2beej2zHnyYV179W8FyV1/3G44ZeWSL4+NOO5mpV11eruZamdTU1PDza/6Nz53xZT5+2ImcddY4DjpoWIsyY0afxLChQxg+4ljOP/8Srv3lfwBw8MEfY+LEL/EPnz6dIz9xMqef9lmGDh1Sia+RPsUbR152JQvkkoZLukTSzyVdk/t8UKnul0bPv/gS+w4awOCB+7DTTjsx5jOj+NOjT21T7rbfzeTkE46h1157tjh+1OEfZ4/de5aptVYuIz95BK+88hqvvvo6mzdv5s47/8Dnzzi1RZkzzjiVW279HQB//svT7LHnHvTv35fhw4fx5z8/zcaNfyeTyTD30acYN3Z0Jb5G+jRF8q3KlCSQS7oEmE52zoG/kH3TScDthebm/bBavWYt/fvu3bzfr28fVq9Z16LMqjVrmTP3Cc4cd1q5m2cVMmBgf95o2PpGd8OyFQwY0L9FmYED+tPwxtYyyxpWMHBAfxYsWMRxx32KXr32Ytddd2HM6JMYNGhA2dqeaplM8q3KlOph50Tg4IjYnH9Q0lXAAuCKQpVyM33VAVz3s8uZ9JWzS9S86lAo1aZW0+38+Jrr+cb551FbW1ueRlnFqfUPARCtfljaKrNo0RKuvPJa7rv3dt579z2efW4hmcbqCzzVKKowZZJUqQJ5EzAAaJ3w3Sd3rqD8GcU2r11aff9+KbJ+ffuwcvWa5v1Vq9eyd5/eLcosWPQy374s+3vvrbc38OiT9dTW1vKZ4z9d1rZa+SxrWMHgvF70oIH7sGLFqhZlGpatYNDgrWUGDtqH5bkyv7l5Or+5eToAl//oUhoaVpSh1V1AFaZMkipVIL8YmCPpZbbOy7svMBS4sET3TJ1Dhh/I6w3LaVi+kn579+beOY/wk8suaVFm9u9ubv78nct/xqhjRjqId3H18+YzdOgQ9ttvMMuWreTMM8dy7ldajly5++77ueD8f+KOO/7A0SOPZMPbG1i5cjUAe+/dmzVr1jF48ADGjRvDscd9vhJfI328+HJLEXGfpAPJTr84kGx+vAGojwj/Oy+nW7da/u83zudr//JdMpkM4z93CkP3/yh3zLgHgLPGn95u/W9fdgX1zzzH+vUb+My4L3PBxHP5QquHYpY+mUyGr1/8XWbdcxu1NTXc/F93sHDhS9R99VwApt1wC7PuncPo0Sex+MXHeX/jRiZN+pfm+r+94wZ69d6LzZsbmTLlO6xf/3alvkq6pLhHrta5t2rxYUitWOftOuC4SjfBqlDjpmWFFnPolPe+NyFxzNnth9N3+H7F5Dc7zczAqRUzs9RLcWrFgdzMDA8/NDNLP/fIzcxSzoHczCzlqvDV+6QcyM3MKO6aneXmQG5mBk6tmJmlnketmJmlnHvkZmYp50BuZpZukUlvasVrdpqZQVGXepM0WtJiSUsKrYomaayk5yTNlzRP0rFJ6xbiHrmZGcUbfiipFrgWOJnc9N2SZkbEwrxic4CZERGSDgXuBIYnrLsN98jNzKCYPfKRwJKIWBoRm8iuXzw2v0BEvBtb5xDfDYikdQtxIDczg+wilAk3SXW5lMiWrS7vSgPZujIaZHvWA1vfTtJ4SYuAe4DzOlO3NadWzMyAaEz+sDN/feECCi06sU03PiJmADMkHQ/8CPhs0rqtOZCbmUE7y8J3WgMwOG9/ELC8rcIRMVfSAZL6dLbuFk6tmJmRfdiZdOtAPTBM0hBJ3YEJwMz8ApKGSlLu85FAd2BdkrqFuEduZgZF65FHRKOkC4HZQC1wU0QskDQ5d34q8AXgK5I2AxuBs3IPPwvW7eieXnzZUsWLL1shxVh8+c3xoxLHnF4zHvHiy2ZmVSe9L3Y6kJuZAURjpVuw/RzIzcyAcI/czCzlHMjNzNLNPXIzs5RzIDczS7nIVNWIwk5xIDczwz1yM7PUiyb3yM3MUs09cjOzlItwj9zMLNXcIzczS7kmj1oxM0s3P+w0M0s5B3Izs5Sr0qUZEmkzkEv6Be0s+hkRU0rSIjOzCuiqPfJ5ZWuFmVmFdcnhhxHxX+VsiJlZJWW68qgVSXsDlwAjgF22HI+Ik0rYLjOzskpzj7wmQZlbgReBIcAPgNeA+hK2ycys7KJJibdqkySQ946IXwObI+KRiDgP+FSJ22VmVlYRybdqk2T44ebcnysknQ4sBwaVrklmZuVXjT3tpJIE8ssl7QF8E/gFsDvwjZK2ysyszDJNSRIU1anDQB4Rd+c+vg2cWNrmmJlVRjWmTJJKMmrlNxR4MSiXKzcz6xKaijhqRdJo4BqgFrgxIq5odf4csqMBAd4Fzo+IZ3PnXgPeATJAY0Qc1dH9kqRW7s77vAswnmye3MysyyjW8ENJtcC1wMlAA1AvaWZELMwr9iowKiLekjQGmAYcnXf+xIhYm/SeSVIrd7Vq5O3Ag0lvYGaWBkVMrYwElkTEUgBJ04GxQHMgj4gn8so/xQ4OINmeSbOGAfvuyE2TOPPIr5f6FpZCG355ZqWbYF1UZ1IrkuqAurxD0yJiWu7zQOCNvHMNtOxttzYRuDdvP4D7JQVwfd5125QkR/4OLXPkK9ma2zEz6xI6M2olF1zbCrCFfiMU7O9LOpFsID827/AxEbFcUl/gAUmLImJue+1Jklrp2VEZM7O0K+KglQZgcN7+IAo8V5R0KHAjMCYi1jW3I2J57s/VkmaQTdW0G8g7/BUkaU6SY2ZmadYUSrx1oB4YJmmIpO7ABGBmfgFJ+wK/B86NiJfyju8mqeeWz8ApwAsd3bC9+ch3AXoAfSTtxdZ/LuwODOjowmZmaVKsUSsR0SjpQmA22eGHN0XEAkmTc+enAt8DegPXSYKtwwz7ATNyx7oBt0XEfR3ds73UyteAi8kG7b+yNZBvIDu0xsysy2gq4rUiYhYwq9WxqXmfJwGTCtRbChzW2fu1Nx/5NcA1ki6KiF909sJmZmkSBZ9RpkOSx7RNkvbcsiNpL0kXlK5JZmbl1xhKvFWbJIH8qxGxfstORLwFfLVkLTIzq4BAibdqk+SFoBpJisi+95R7/bR7aZtlZlZexcyRl1uSQD4buFPSVLJDLSfT8i0kM7PUq8aedlJJAvklZF9FPZ/syJVngH1K2Sgzs3Lr0j3yiGiS9BSwP3AW0Au4q/1aZmbpkumKPXJJB5J9I+lsYB1wB0BEeHEJM+tyUrzSW7s98kXAo8AZEbEEQJKXeDOzLqkpxT3y9oYffoHsTIcPSbpB0mcoPKuXmVnqRSe2atNmII+IGRFxFjAceJjsgsv9JP1K0illap+ZWVk0dWKrNh2+EBQR70XErRHxObLTMc4HLi11w8zMyqlJSrxVm+QzqQMR8WZEXB8RJ5WqQWZmlZDpxFZttmepNzOzLqerjloxM/vQSPOoFQdyMzOqczRKUg7kZmY4tWJmlnrVOKwwKQdyMzMg4x65mVm6uUduZpZyDuRmZilXhUtxJuZAbmaGe+RmZqlXja/eJ+VAbmZGuseRd2rSLDOzrqqY09hKGi1psaQlkraZLVbSOZKey21PSDosad1CHMjNzCheIJdUC1wLjAFGAGdLGtGq2KvAqIg4FPgRMK0TdbfhQG5mRlFXCBoJLImIpRGxCZgOjG1xr4gnIuKt3O5TZNd6SFS3EAdyMzOyOfKkm6Q6SfPytrq8Sw0E3sjbb8gda8tE4N7trAv4YaeZGdC5USsRMY1cOqSAQo9NC3bkJZ1INpAf29m6+RzIzcyApuJNZNsADM7bHwQsb11I0qHAjcCYiFjXmbqtObViZkZRR63UA8MkDZHUHZgAzMwvIGlf4PfAuRHxUmfqFuIeuZkZxVtYIiIaJV0IzAZqgZsiYoGkybnzU4HvAb2B65RdzLkxIo5qq25H93QgNzOjuK/oR8QsYFarY1PzPk8CJiWt2xEHcjMzoFHpXezNgdzMDK/ZaWaWep790Mws5Yo4/LDsHMjNzHBqxcws9ZxaMTNLuUyK++QO5GZmuEduZpZ64R65mVm6uUdu2+2IUUcy8ftfpaa2hgenP8Dvr/tdi/MDDxjERT/9OvsfcgC3XnkLf5g2o/lcj913459/chH7HvhRiOCX376GxU8vLvdXsBJ4/NU1XPnQQpoiGHfIYM47+oAW5x9asopfPf4SEtTWiG+fMIIjBvVqPp9pCs7578fp23Nnfj7+k+Vufip5+KFtl5qaGuoun8z3z/lX1q1Yx0/+eBV/eeDPNLy8dV75d9e/w42XTePoUz+1Tf1J3/8qzzz8NFdOvoJuO3Wj+647l7P5ViKZpuCKOQv41RdH0q/nLpxz6+OMGtqXA3r3bC5z9L69OeGAY5HES2s2cMkfn2HGeaOaz9/29KsM6b0b721qrMRXSKX0hnFPY1tRww4fxorXVrDq9VU0bm7ksT/OZeQpR7co8/a6t1ny3Ms0Nrb8C7nrR3ZlxMhDeHD6/QA0bm7k/Q3vla3tVjovrFzP4D17MGjPHuxUW8OpH9uHh5esalGmR/du5GbNY+PmDMpbjmDVOxt57NU1jP/4YCy5RiLxVm3cI6+gXv17s3b52ub9dSvWceDhByaq22/f/mx4820u+tnF7HfQfrzy/Cv8+vvT+GDjB6VqrpXJ6nf/Tr+euzTv9+u5Ky+sWL9NuT+9vJJfPLqYNzdu4ufjj2o+fuVDL/L144fzvnvjnZLmh51l75FL+t/tnGteB++1d/9WzmZVhLTtqk4RyX6YarvVsv8hB3DfLbP45mkX88HGv/OPF3yx2E20SkgYT04a1p8Z543iqrGf4LrHs2sTzH1lFb16dGdEvz1K2MCuqYgLS5RdJVIrP2jrRERMy02uftR+H/loOdtUEetWrKXPgD7N+7336c2bq99MXHfdirW8PD/7F/iJWY+z/yEHdFDL0qBvz11Y9c7fm/dXvbORvT/S9vOPTwzqRcP693nr/U3MX/4Wj7yymtNueIhL736G+tfX8Z1Z88vQ6vSLTvxXbUqSWpH0XFungH6luGcavfzsy+wzZAB9B/fjzZXrOPaM47l6yk8T1V2/Zj1rV6xlwP4DWb50GYcec1iLh6SWXgf334PX17/Hsrffp+9HdmH24hX8x2mHtyjz+lvvMXjPHkjixVVvs7mpiT133Ykpxw1nynHDAZj3xjr+37yl/FurulZYNfa0kypVjrwfcCrwVqvjAp4o0T1TpynTxA3/OpXLbvkBNbU1zLnjQd546XVO/fJoAGb/933sufeeXHn31fT4SA+iqYnPTfw8Uz5zARvf3cgN37ueb/z8m3TbqRurXl/FL771n5X9QlYU3WpquOSkg7ngrr/Q1ARjDxnEAX168ttns+nG/3XYR5nz8kruXriMbjVi5261/Pj0Iwqm6iy5TMK0ZjVS0pxspy4q/Rr4TUQ8VuDcbRHxpY6uMX7fM9L7f9VK5tbvDq10E6wK9ai7eod/i33po+MTx5zb/jajqn5rlqRHHhET2znXYRA3Myu3asx9J+Xhh2ZmOEduZpZ6fkXfzCzlnFoxM0u5NI9acSA3MyPdqRVPmmVmRnFf0Zc0WtJiSUskXVrg/HBJT0r6QNK3Wp17TdLzkuZLmpek7e6Rm5lRvBy5pFrgWuBkoAGolzQzIhbmFXsTmAKMa+MyJ0bE2jbObcM9cjMzsqmVpFsHRgJLImJpRGwCpgNj8wtExOqIqAc2F6PtDuRmZmRnHk265c/Umtvq8i41EMif+KghdyxxU4D7Jf211XXb5NSKmRmQ6URqJSKmAdPaOF3o9f3O5G2OiYjlkvoCD0haFBFz26vgHrmZGUVNrTQA+cszDQKWJ21HRCzP/bkamEE2VdMuB3IzMzqXWulAPTBM0hBJ3YEJwMwkbZC0m6SeWz4DpwAvdFTPqRUzM4o3jjwiGiVdCMwGaoGbImKBpMm581Ml9QfmAbsDTZIuBkYAfYAZuSmJuwG3RcR9Hd3TgdzMjOK+oh8Rs4BZrY5Nzfu8kmzKpbUNwGGdvZ8DuZkZfkXfzCz10vyKvgO5mRkO5GZmqVeKZS/LxYHczAz3yM3MUs8LS5iZpVwm0rtqpwO5mRnOkZuZpZ5z5GZmKeccuZlZyjU5tWJmlm7ukZuZpZxHrZiZpZxTK2ZmKefUiplZyrlHbmaWcu6Rm5mlXCYylW7CdnMgNzPDr+ibmaWeX9E3M0s598jNzFLOo1bMzFLOo1bMzFLOr+ibmaVcmnPkNZVugJlZNWiKSLx1RNJoSYslLZF0aYHzwyU9KekDSd/qTN1C3CM3M6N4PXJJtcC1wMlAA1AvaWZELMwr9iYwBRi3HXW34R65mRnZceRJtw6MBJZExNKI2ARMB8bmF4iI1RFRD2zubN1CHMjNzMj2yJNukuokzcvb6vIuNRB4I2+/IXcsie2q69SKmRmdG7USEdOAaW2cVqEqCS+9XXUdyM3MKOoLQQ3A4Lz9QcDyUtZ1asXMjM6lVjpQDwyTNERSd2ACMDNhM7arrnvkZmYU783OiGiUdCEwG6gFboqIBZIm585PldQfmAfsDjRJuhgYEREbCtXt6J4O5GZmFPeFoIiYBcxqdWxq3ueVZNMmiep2xIHczIx0T5qlNL+W+mEhqS73lNysmX8ubAs/7EyHuo6L2IeQfy4McCA3M0s9B3Izs5RzIE8H50GtEP9cGOCHnWZmqeceuZlZyjmQm5mlnAN5ldue1UKsa5N0k6TVkl6odFusOjiQV7G81ULGACOAsyWNqGyrrArcDIyudCOsejiQV7ftWi3EuraImEt2qTAzwIG82u3ISiNm9iHhQF7ddmSlETP7kHAgr247stKImX1IOJBXtx1ZacTMPiQcyKtYRDQCW1YLeRG4M8lqIda1SbodeBL4mKQGSRMr3SarLL+ib2aWcu6Rm5mlnAO5mVnKOZCbmaWcA7mZWco5kJuZpZwDuZWEpIyk+ZJekPRbST124Fo3S/pi7vON7U0cJukESZ/ejnu8JqnP9rbRrJIcyK1UNkbE4RFxCLAJmJx/MjezY6dFxKSIWNhOkROATgdyszRzILdyeBQYmustPyTpNuB5SbWSrpRUL+k5SV8DUNYvJS2UdA/Qd8uFJD0s6ajc59GSnpb0rKQ5kvYj+wvjG7l/DRwnaW9Jd+XuUS/pmFzd3pLul/SMpOspPK+NWSp0q3QDrGuT1I3sfOr35Q6NBA6JiFcl1QFvR8QnJe0MPC7pfuAI4GPAx4F+wELgplbX3Ru4ATg+d61eEfGmpKnAuxHx01y524CrI+IxSfuSfUv2IOAy4LGI+KGk04G6kv6PMCshB3IrlV0lzc99fhT4NdmUx18i4tXc8VOAQ7fkv4E9gGHA8cDtEZEBlkv6U4HrfwqYu+VaEdHW/NyfBUZIzR3u3SX1zN3jH3N175H01vZ9TbPKcyC3UtkYEYfnH8gF0/fyDwEXRcTsVuVOo+PpepWgDGTTh/8QERsLtMXzU1iX4By5VdJs4HxJOwFIOlDSbsBcYEIuh74PcGKBuk8CoyQNydXtlTv+DtAzr9z9ZCceI1fu8NzHucA5uWNjgL2K9aXMys2B3CrpRrL576dzCwlfT/ZfiTOAl4HngV8Bj7SuGBFryOa1fy/pWeCO3Kk/AuO3POwEpgBH5R6mLmTr6JkfAMdLeppsiuf1En1Hs5Lz7IdmZinnHrmZWco5kJuZpZwDuZlZyjmQm5mlnAO5mVnKOZCbmaWcA7mZWcr9f0AOTQpVSUuLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get actual and create heatmap \n",
    "confusion_matrix = pd.crosstab(actual, predictions , rownames=['Actual'], colnames=['Predicted'], normalize=True)\n",
    "plt.subplots(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(g): Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way to put the data - it gets the same results\n",
    "# train_x_ridge = df_train['feature_vector_array']\n",
    "# test_x_ridge = df_test['feature_vector_array']\n",
    "\n",
    "# train_x_ridge = pd.DataFrame(train_x_ridge.to_list()).to_numpy()\n",
    "# test_x_ridge = pd.DataFrame(test_x_ridge.to_list()).to_numpy()\n",
    "\n",
    "# train_y_ridge = train_Y\n",
    "# test_y_ridge = test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_list = list(word_freq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_feat_vect = pd.DataFrame(df_train['feature_vector_array'].to_list())\n",
    "train_X_feat_vect.columns = list(word_freq.keys())\n",
    "test_X_feat_vect = pd.DataFrame(df_test['feature_vector_array'].to_list())\n",
    "test_X_feat_vect.columns = list(word_freq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = LogisticRegressionCV(cv=10, penalty='l2', solver='liblinear').fit(train_X_feat_vect, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ridge.predict(test_X_feat_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ridge.score(test_X_feat_vect, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most and least important words for the ridge resgression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge.coef_\n",
    "coeffs_ridge = list(np.argsort(model_ridge.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "love\n",
      "delicious\n",
      "excellent\n",
      "fantastic\n",
      "nice\n",
      "awesome\n",
      "loved\n",
      "happier\n",
      "liked\n"
     ]
    }
   ],
   "source": [
    "# most important words:\n",
    "most_important = coeffs_ridge[0][::-1]\n",
    "for i in range(0,10):\n",
    "    print(unique_words_list[most_important[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poor\n",
      "bad\n",
      "disappointment\n",
      "not\n",
      "worst\n",
      "average\n",
      "slow\n",
      "awful\n",
      "terrible\n",
      "disappointing\n"
     ]
    }
   ],
   "source": [
    "least_important = coeffs_ridge[0]\n",
    "for i in range(0,10):\n",
    "    print(unique_words_list[least_important[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aliyagangji/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model_lasso = LogisticRegressionCV(cv=10, penalty='l1', solver='liblinear').fit(train_X_feat_vect, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lasso.predict(test_X_feat_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lasso.score(test_X_feat_vect, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a dictionary of n-gram (n=2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>level 5 spicy perfect spice overwhelm soup</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good quality bargain bought bought cheapy big ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love ownerchef one authentic japanese cool dude</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excellent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pairing iphone could not happier far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>drain weak snap</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>flair bartender absolutely amazing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>however much garlic fondue barely edible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>pizza selection good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>my order not correct</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label\n",
       "0           level 5 spicy perfect spice overwhelm soup      1\n",
       "1    good quality bargain bought bought cheapy big ...      1\n",
       "2      love ownerchef one authentic japanese cool dude      1\n",
       "3                                            excellent      1\n",
       "4                 pairing iphone could not happier far      1\n",
       "..                                                 ...    ...\n",
       "595                                    drain weak snap      0\n",
       "596                 flair bartender absolutely amazing      1\n",
       "597           however much garlic fondue barely edible      0\n",
       "598                               pizza selection good      1\n",
       "599                               my order not correct      0\n",
       "\n",
       "[600 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all two consecutive words in all rows in the dataframe: \n",
    "list_consecutive = []\n",
    "for row in range(0, len(df_train['sentence'])):\n",
    "    sentence = df_train['sentence'][row]\n",
    "    word_list = df_train['sentence'][row].split()\n",
    "    i=0\n",
    "    while i < len(word_list)-1:\n",
    "        if (word_list[i] + ' ' + word_list[i+1]) not in list_consecutive:\n",
    "            list_consecutive.append(word_list[i]+ ' ' + word_list[i+1])\n",
    "            i+=1\n",
    "        else:\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(list_consecutive) #self-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of consecutive into a dictionary\n",
    "ngram_dict = dict.fromkeys(list_consecutive, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat 2(d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that return the two consecutive for every row in the dataframe:\n",
    "def consecutive(sentence):\n",
    "    list_cons_per_row = []\n",
    "    word_list = sentence.split()\n",
    "    i=0\n",
    "    while i < len(word_list)-1:\n",
    "        if (word_list[i] + ' ' + word_list[i+1]) not in list_cons_per_row:\n",
    "            list_cons_per_row.append(word_list[i]+ ' ' + word_list[i+1])\n",
    "            i+=1\n",
    "        else:\n",
    "            i+=1\n",
    "    return(list_cons_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_list_3 = consecutive(df_train['sentence'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in word_list_3:\n",
    "#     print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram_dict['punishment park']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11746"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngram_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vector_func_ngram(sentence):\n",
    "    feature_vector_row = dict.fromkeys(ngram_dict, 0)\n",
    "    word_list = consecutive(sentence)\n",
    "    for word in word_list:\n",
    "        if word in ngram_dict:\n",
    "            feature_vector_row[word] = feature_vector_row[word] + 1\n",
    "        else:\n",
    "            pass\n",
    "    return(feature_vector_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ngram['feature_vector_dict_ngram'] = df_train_ngram['sentence'].apply(lambda x: feature_vector_func_ngram(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not wanted</td>\n",
       "      <td>0</td>\n",
       "      <td>{'not wanted': 1, 'pulled car': 0, 'car waited...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pulled car waited another 15 minute acknowledged</td>\n",
       "      <td>0</td>\n",
       "      <td>{'not wanted': 0, 'pulled car': 1, 'car waited...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>today first taste buldogis gourmet hot dog tel...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not wanted': 0, 'pulled car': 0, 'car waited...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>place not worth time let alone vega</td>\n",
       "      <td>0</td>\n",
       "      <td>{'not wanted': 0, 'pulled car': 0, 'car waited...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>easy use</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not wanted': 0, 'pulled car': 0, 'car waited...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0                                         not wanted      0   \n",
       "1   pulled car waited another 15 minute acknowledged      0   \n",
       "2  today first taste buldogis gourmet hot dog tel...      1   \n",
       "3                place not worth time let alone vega      0   \n",
       "4                                           easy use      1   \n",
       "\n",
       "                           feature_vector_dict_ngram  \n",
       "0  {'not wanted': 1, 'pulled car': 0, 'car waited...  \n",
       "1  {'not wanted': 0, 'pulled car': 1, 'car waited...  \n",
       "2  {'not wanted': 0, 'pulled car': 0, 'car waited...  \n",
       "3  {'not wanted': 0, 'pulled car': 0, 'car waited...  \n",
       "4  {'not wanted': 0, 'pulled car': 0, 'car waited...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ngram.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the function to all rows in the same manner for df_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_ngram['feature_vector_dict_ngram'] = df_test_ngram['sentence'].apply(lambda x: feature_vector_func_ngram(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>level 5 spicy perfect spice overwhelm soup</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not wanted': 0, 'pulled car': 0, 'car waited...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good quality bargain bought bought cheapy big ...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not wanted': 0, 'pulled car': 0, 'car waited...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love ownerchef one authentic japanese cool dude</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not wanted': 0, 'pulled car': 0, 'car waited...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excellent</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not wanted': 0, 'pulled car': 0, 'car waited...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pairing iphone could not happier far</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not wanted': 0, 'pulled car': 0, 'car waited...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0         level 5 spicy perfect spice overwhelm soup      1   \n",
       "1  good quality bargain bought bought cheapy big ...      1   \n",
       "2    love ownerchef one authentic japanese cool dude      1   \n",
       "3                                          excellent      1   \n",
       "4               pairing iphone could not happier far      1   \n",
       "\n",
       "                           feature_vector_dict_ngram  \n",
       "0  {'not wanted': 0, 'pulled car': 0, 'car waited...  \n",
       "1  {'not wanted': 0, 'pulled car': 0, 'car waited...  \n",
       "2  {'not wanted': 0, 'pulled car': 0, 'car waited...  \n",
       "3  {'not wanted': 0, 'pulled car': 0, 'car waited...  \n",
       "4  {'not wanted': 0, 'pulled car': 0, 'car waited...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_ngram.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare any two feature vectors of any two reviews in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'not wanted': 1, 'pulled car': 0, 'car waited...\n",
       "1    {'not wanted': 0, 'pulled car': 1, 'car waited...\n",
       "Name: feature_vector_dict_ngram, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ngram['feature_vector_dict_ngram'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat 2(e) - Postprocessing Strategy with ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ngram['feature_vector_array_ngram'] = df_train_ngram['feature_vector_dict_ngram'].apply(lambda x: dict_to_array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict_ngram</th>\n",
       "      <th>feature_vector_array_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not wanted</td>\n",
       "      <td>0</td>\n",
       "      <td>{'not wanted': 1, 'pulled car': 0, 'car waited...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pulled car waited another 15 minute acknowledged</td>\n",
       "      <td>0</td>\n",
       "      <td>{'not wanted': 0, 'pulled car': 1, 'car waited...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>today first taste buldogis gourmet hot dog tel...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not wanted': 0, 'pulled car': 0, 'car waited...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>place not worth time let alone vega</td>\n",
       "      <td>0</td>\n",
       "      <td>{'not wanted': 0, 'pulled car': 0, 'car waited...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>easy use</td>\n",
       "      <td>1</td>\n",
       "      <td>{'not wanted': 0, 'pulled car': 0, 'car waited...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0                                         not wanted      0   \n",
       "1   pulled car waited another 15 minute acknowledged      0   \n",
       "2  today first taste buldogis gourmet hot dog tel...      1   \n",
       "3                place not worth time let alone vega      0   \n",
       "4                                           easy use      1   \n",
       "\n",
       "                           feature_vector_dict_ngram  \\\n",
       "0  {'not wanted': 1, 'pulled car': 0, 'car waited...   \n",
       "1  {'not wanted': 0, 'pulled car': 1, 'car waited...   \n",
       "2  {'not wanted': 0, 'pulled car': 0, 'car waited...   \n",
       "3  {'not wanted': 0, 'pulled car': 0, 'car waited...   \n",
       "4  {'not wanted': 0, 'pulled car': 0, 'car waited...   \n",
       "\n",
       "                          feature_vector_array_ngram  \n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ngram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ngram['standard_array_ngram'] = df_train_ngram['feature_vector_array_ngram'].apply(lambda x: standard(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_ngram['feature_vector_array_ngram'] = df_test_ngram['feature_vector_dict_ngram'].apply(lambda x: dict_to_array(x))\n",
    "df_test_ngram['standard_array_ngram'] = df_test_ngram['feature_vector_array_ngram'].apply(lambda x: standard(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11746"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test_ngram['standard_array_ngram'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reapeat 2(f) - Sentiment Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a naive Bayes model on the training set and test on the testing set. \n",
    "\n",
    "Report the classification accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ngram['standard_array_clean_ngram'] = df_train_ngram['standard_array_ngram'].apply(lambda x: array_of_arrays(x))\n",
    "df_test_ngram['standard_array_clean_ngram'] = df_test_ngram['standard_array_ngram'].apply(lambda x: array_of_arrays(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11746"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test_ngram['standard_array_clean_ngram'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_ngram = df_train_ngram[\"standard_array_clean_ngram\"]\n",
    "test_X_ngram = df_test_ngram[\"standard_array_clean_ngram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Check that there is no \"Nan\" values:\n",
    "for array in test_X_ngram:\n",
    "    array_sum = np.sum(array)\n",
    "    array_has_nan = np.isnan(array_sum)\n",
    "print(array_has_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#False = no \"Nan\" values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_ngram = pd.DataFrame(train_X_ngram.to_list())\n",
    "# train_X_ngram = train_X_ngram.loc[:, :3753]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Check again - no \"Nan\" values\n",
    "for array in test_X_ngram:\n",
    "    array_sum = np.sum(array)\n",
    "    array_has_nan = np.isnan(array_sum)\n",
    "print(array_has_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_ngram = train_X_ngram.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11746"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_ngram.shape[1] #works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11736</th>\n",
       "      <th>11737</th>\n",
       "      <th>11738</th>\n",
       "      <th>11739</th>\n",
       "      <th>11740</th>\n",
       "      <th>11741</th>\n",
       "      <th>11742</th>\n",
       "      <th>11743</th>\n",
       "      <th>11744</th>\n",
       "      <th>11745</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 11746 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6      \\\n",
       "0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1   -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227   \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4   -0.013050 -0.013050 -0.013050 -0.013050 -0.013050 -0.013050 -0.013050   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "595  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "596 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227   \n",
       "597  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "598 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227   \n",
       "599  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        7         8         9      ...     11736     11737     11738  \\\n",
       "0    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1   -0.009227 -0.009227 -0.009227  ... -0.009227 -0.009227 -0.009227   \n",
       "2    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "4   -0.013050 -0.013050 -0.013050  ... -0.013050 -0.013050 -0.013050   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "595  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "596 -0.009227 -0.009227 -0.009227  ... -0.009227 -0.009227 -0.009227   \n",
       "597  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "598 -0.009227 -0.009227 -0.009227  ... -0.009227 -0.009227 -0.009227   \n",
       "599  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "        11739     11740     11741     11742     11743     11744     11745  \n",
       "0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1   -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227  \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4   -0.013050 -0.013050 -0.013050 -0.013050 -0.013050 -0.013050 -0.013050  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "595  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "596 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227  \n",
       "597  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "598 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227  \n",
       "599  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[600 rows x 11746 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_ngram = pd.DataFrame(test_X_ngram.to_list())\n",
    "test_X_ngram #I see there is redundant data - drop all columns after 3756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11736</th>\n",
       "      <th>11737</th>\n",
       "      <th>11738</th>\n",
       "      <th>11739</th>\n",
       "      <th>11740</th>\n",
       "      <th>11741</th>\n",
       "      <th>11742</th>\n",
       "      <th>11743</th>\n",
       "      <th>11744</th>\n",
       "      <th>11745</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.013050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.009227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 11746 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6      \\\n",
       "0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1   -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227   \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4   -0.013050 -0.013050 -0.013050 -0.013050 -0.013050 -0.013050 -0.013050   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "595  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "596 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227   \n",
       "597  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "598 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227   \n",
       "599  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        7         8         9      ...     11736     11737     11738  \\\n",
       "0    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1   -0.009227 -0.009227 -0.009227  ... -0.009227 -0.009227 -0.009227   \n",
       "2    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "4   -0.013050 -0.013050 -0.013050  ... -0.013050 -0.013050 -0.013050   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "595  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "596 -0.009227 -0.009227 -0.009227  ... -0.009227 -0.009227 -0.009227   \n",
       "597  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "598 -0.009227 -0.009227 -0.009227  ... -0.009227 -0.009227 -0.009227   \n",
       "599  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "        11739     11740     11741     11742     11743     11744     11745  \n",
       "0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1   -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227  \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4   -0.013050 -0.013050 -0.013050 -0.013050 -0.013050 -0.013050 -0.013050  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "595  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "596 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227  \n",
       "597  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "598 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227 -0.009227  \n",
       "599  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[600 rows x 11746 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping redundant columns\n",
    "test_X_ngram   #= test_X_ngram.loc[:, :3755]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#check there is no \"Nan\" values\n",
    "for array in test_X_ngram:\n",
    "    array_sum = np.sum(array)\n",
    "    array_has_nan = np.isnan(array_sum)\n",
    "print(array_has_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#False - we can continue:\n",
    "test_X_ngram = test_X_ngram.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 11746)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_ngram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y_ngram = df_train['label']\n",
    "test_Y_ngram = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = train_X_ngram.shape[0] #number of rows\n",
    "d = train_X_ngram.shape[1] #number of unique words = features in feature vector\n",
    "K = 2 #number of classes - label 1 or label 0\n",
    "\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    X_k_ngram = train_X_ngram[train_Y_ngram == k]\n",
    "    phis[k] = X_k_ngram.shape[0] / float(n)\n",
    "    psis[k] = np.mean(X_k_ngram, axis=0) #build a function with the mean for label 0 and mean for label 0 as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 11746) (2,) (2400, 11746)\n"
     ]
    }
   ],
   "source": [
    "print(psis.shape, phis.shape, train_X_ngram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement this in numpy\n",
    "def nb_predictions(x, psis, phis):\n",
    "    \"\"\"This returns class assignments and scores under the NB model.\n",
    "    \n",
    "    We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n",
    "    \"\"\"\n",
    "    # adjust shapes\n",
    "    n , d = x.shape\n",
    "    x = np.reshape(x, (1,n,d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14) #understand\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape(K,1)\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes on the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train_ngram, logpyx_ngram = nb_predictions(train_X_ngram, psis, phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in predicted_train_ngram:\n",
    "#     if i != 0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9883333333333333"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_train_ngram == train_Y_ngram).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_ngram, logpyx_test = nb_predictions(test_X_ngram, psis, phis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6633333333333333"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_test_ngram == test_Y_ngram).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ngram = test_Y_ngram\n",
    "predictions_ngram = predicted_test_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_ngram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_ngram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Predicted', ylabel='Actual'>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get actual and create heatmap \n",
    "confusion_matrix_ngram = pd.crosstab(actual_ngram, predictions_ngram , rownames=['Actual'], colnames=['Predicted'], normalize=True)\n",
    "plt.subplots(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix_ngram, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat 2(g): Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_feat_vect_ngram = pd.DataFrame(df_train_ngram['feature_vector_array_ngram'].to_list())\n",
    "train_X_feat_vect_ngram.columns = list(ngram_dict.keys())\n",
    "test_X_feat_vect_ngram = pd.DataFrame(df_test_ngram['feature_vector_array_ngram'].to_list())\n",
    "# test_X_feat_vect_ngram = test_X_feat_vect_ngram.loc[:, :3755]\n",
    "test_X_feat_vect_ngram.columns = list(ngram_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge_ngram = LogisticRegressionCV(cv=10, penalty='l2', solver='liblinear').fit(train_X_feat_vect_ngram, train_Y_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge_ngram.predict(test_X_feat_vect_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge_ngram.score(test_X_feat_vect_ngram, test_Y_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most and least important words for the ridge resgression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge_ngram.coef_\n",
    "coeffs_ridge_ngram = list(np.argsort(model_ridge_ngram.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ngram = list(ngram_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important words:\n",
    "most_important_ngram = coeffs_ridge_ngram[0][::-1]\n",
    "for i in range(0,10):\n",
    "    print(list_ngram[most_important[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_important_ngram = coeffs_ridge_ngram[0]\n",
    "for i in range(0,10):\n",
    "    print(list_ngram[least_important[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso_ngram = LogisticRegressionCV(cv=10, penalty='l1', solver='liblinear').fit(train_X_feat_vect_ngram, train_Y_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso_ngram.predict(test_X_feat_vect_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso_ngram.score(test_X_feat_vect_ngram, test_Y_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Comparison and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above results, compare the performances of naive Bayes, logistic regression, naive Bayes with 2-grams, and logistic regression\n",
    "with 2-grams. Which method performs best in the prediction task and why? What do you\n",
    "learn about the language that people use in online reviews (e.g., expressions that will make\n",
    "the posts positive/negative)? Hint: Inspect the weights learned from logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general - part (1) is better than part (2).\n",
    "\n",
    "In n-gram model:\n",
    "    lasso accuracy < ridge accuracy because Lasso penalizes for many features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE - QUESTIONS FOR ME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso & Ridge - do twice the importance of words or once? weird results for lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso.coef_\n",
    "coeffs_lasso = list(np.argsort(model_lasso.coef_))\n",
    "# print(coeffs_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important words:\n",
    "most_important_lasso = coeffs_lasso[0][::-1]\n",
    "for i in range(0,10):\n",
    "    print(unique_words_list[most_important[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_important = coeff[0]\n",
    "for i in range(0,10):\n",
    "    print(unique_words_list[least_important[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  *** Why is all of that not giving me good scores? How is it differentthan the ridge part? *** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing alpha vs. not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression with 10-fold cross validation to chose ¸\n",
    "# The Ridge() function has an alpha argument ( λ , but with a different name!) that is used to tune the model\n",
    "# Alpha values ranging from very big to very small, essentially covering the full \n",
    "# range of scenarios from the null model containing only the intercept, to the least squares fit\n",
    "\n",
    "alphas = 10**np.linspace(10,-2,100)*0.5 \n",
    "\n",
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(train_x_ridge, train_y_ridge)\n",
    "ridgecv.alpha_\n",
    "\n",
    "ridge = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "ridge.fit(train_x_ridge, train_y_ridge)\n",
    "mean_squared_error(test_y_ridge, ridge.predict(test_x_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression with 10-fold cross validation to chose ¸\n",
    "# The Ridge() function has an alpha argument ( λ , but with a different name!) that is used to tune the model\n",
    "# Alpha values ranging from very big to very small, essentially covering the full \n",
    "# range of scenarios from the null model containing only the intercept, to the least squares fit\n",
    "\n",
    "alphas = 10**np.linspace(10,-2,100)*0.5 \n",
    "\n",
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(train_X_feat_vect, train_Y)\n",
    "ridgecv.alpha_\n",
    "\n",
    "ridge = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "ridge.fit(train_X_feat_vect, train_Y)\n",
    "mean_squared_error(test_Y, ridge.predict(test_X_feat_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge.predict(test_x_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.score(train_x_ridge, train_y_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgereg = Ridge(alpha=1.0,normalize=True)\n",
    "ridgereg.fit(train_x_ridge, train_y_ridge)\n",
    "y_predict = ridgereg.predict(train_x_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is this different than the other ridge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RidgeClassifier().fit(train_x_ridge, train_y_ridge)\n",
    "clf.score(train_x_ridge, train_y_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RidgeClassifier().fit(test_x_ridge, test_y_ridge)       # clf = Ridge(alpha=1.0)\n",
    "clf.score(test_x_ridge, test_y_ridge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
