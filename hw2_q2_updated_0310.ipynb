{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ophir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ophir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ophir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import sklearn.neighbors\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#lemmatization libraries \n",
    "import nltk.stem\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# punctuation + stop words libraries:\n",
    "from nltk.corpus import stopwords\n",
    "import re, string, timeit\n",
    "\n",
    "#stemming \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1(a) Parsing the txt files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1(a): Parsing Yelp, IMDB, and Amazon database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = pd.read_csv(\"yelp_labelled.txt\", sep=\"\\t\", header=None, names=[\"sentence\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many values are there? How many positive and negative emotions (0/1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-0a2494051e90>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df_imdb = pd.read_csv(\"imdb_labelled.txt\", sep=\" \\t\", header=None, names=[\"sentence\", \"label\"])\n"
     ]
    }
   ],
   "source": [
    "df_imdb = pd.read_csv(\"imdb_labelled.txt\", sep=\" \\t\", header=None, names=[\"sentence\", \"label\"])\n",
    "#Notice: IMDB required another space in order to parse the 1000 lines because it was badly formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many values are there? How many positive and negative emotions (0/1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imdb.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Amazon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon = pd.read_csv(\"amazon_cells_labelled.txt\", sep=\"\\t\", header=None, names=[\"sentence\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many values are there? How many positive and negative emotions (0/1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results - Amazon, IMDB and Yelp are balanced (50% negative 50% positive)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(b): Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick your preprocessing strategy. Since these sentences are online reviews, they may contain significant amounts of noise and garbage. You may or may not want to do one or all of\n",
    "the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Lowercase all of the words.\n",
    "\n",
    "• Lemmatization of all the words (i.e., convert every word to its root so that all of “running,”\n",
    "“run,” and “runs” are converted to “run” and and all of “good,” “well,” “better,” and “best”\n",
    "are converted to “good”; this is easily done using nltk.stem).\n",
    "\n",
    "• Strip punctuation.\n",
    "\n",
    "• Strip the stop words, e.g., “the”, “and”, “or”.\n",
    "\n",
    "• Something else? Tell us about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase all databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[\"sentence\"] = df_yelp[\"sentence\"].str.lower()\n",
    "df_imdb[\"sentence\"] = df_imdb[\"sentence\"].str.lower()\n",
    "df_amazon[\"sentence\"] = df_amazon[\"sentence\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = nltk.tokenize.WhitespaceTokenizer() \n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize(text):\n",
    "#     return [lemmatizer.lemmatize(w) for w in word_tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    list2 = nltk.word_tokenize(text)\n",
    "    lemmatized_sentence = ' '.join([lemmatizer.lemmatize(words) for words in list2])\n",
    "    return(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['sentence'] = df_yelp['sentence'].apply(lambda x: lemmatize_text(x))\n",
    "df_imdb['sentence'] = df_imdb['sentence'].apply(lambda x: lemmatize_text(x))\n",
    "df_amazon['sentence'] = df_amazon['sentence'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuation Strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that removes punctuation:\n",
    "def remove_punctuation(sentence):\n",
    "    sentence = re.sub(r'[^\\w\\s]','',sentence)\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other versions in case we need\n",
    "# def remove_punctuation(sentence):\n",
    "#     tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "#     return [tokenizer.tokenize(w) for w in word_tokenizer.tokenize(sentence)]\n",
    "\n",
    "# def join_sentence(text):\n",
    "#     sentence = ' '.join(text)\n",
    "#     sentence = sentence.replace(string.punctuation, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[\"sentence\"] = df_yelp['sentence'].apply(lambda x: remove_punctuation(x))\n",
    "df_imdb[\"sentence\"] = df_imdb['sentence'].apply(lambda x: remove_punctuation(x))\n",
    "df_amazon[\"sentence\"] = df_amazon['sentence'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strip the stop words, e.g., “the”, “and”, “or”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['sentence'] = df_yelp['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df_imdb['sentence'] = df_imdb['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df_amazon['sentence'] = df_amazon['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure there are no stop words in the database:\n",
    "# df_yelp[\"sentence\"].str.contains(\"the\", na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 (extra): stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to add stemming because it is helps us to achieve the root forms (synonyms) of inflected words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    w_list = text.split()\n",
    "    stem_list = []\n",
    "    for word in w_list:\n",
    "        stemmed_word = porter.stem(word)\n",
    "        stem_list.append(stemmed_word)\n",
    "    stemmed_sentence = ' '.join(stem_list)\n",
    "    return(stemmed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply \"stemming\" on the \"sentence\" column on all datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['sentence'] = df_yelp['sentence'].apply(lambda x: stemming(x))\n",
    "df_imdb['sentence'] = df_yelp['sentence'].apply(lambda x: stemming(x))\n",
    "df_amazon['sentence'] = df_yelp['sentence'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2(C): Split training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each file use the first 400 instances for each label as the training set and the remaining 100 instances as testing set.\n",
    "\n",
    "In total, there are 2400 reviews for training and 600 reviews for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yelp - Divide to positive and negative data set and extract from there the train / test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into positive / negative datasets:\n",
    "df_yelp_pos = df_yelp[df_yelp['label'] == 1].reset_index(drop=True)   # positive emotion = 1\n",
    "df_yelp_neg = df_yelp[df_yelp['label'] == 0].reset_index(drop=True)   # Negative emotion = 0\n",
    "\n",
    "# For the positive dataset - divide into train / test datasets:\n",
    "df_yelp_train_pos = df_yelp_pos[:400]\n",
    "df_yelp_test_pos = df_yelp_pos[400:500]\n",
    "\n",
    "# For the negative dataset - divide into train / test datasets:\n",
    "df_yelp_train_neg = df_yelp_neg[:400]\n",
    "df_yelp_test_neg = df_yelp_neg[400:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB - Divide to positive and negative data set and extract from there the train / test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into positive / negative datasets:\n",
    "df_imdb_pos = df_imdb[df_imdb['label'] == 1].reset_index(drop=True)   # positive emotion = 1\n",
    "df_imdb_neg = df_imdb[df_imdb['label'] == 0].reset_index(drop=True)   # Negative emotion = 0\n",
    "\n",
    "# For positive - train / test datasets:\n",
    "df_imdb_train_pos = df_imdb_pos[:400]\n",
    "df_imdb_test_pos = df_imdb_pos[400:500]\n",
    "\n",
    "# For the negative dataset - divide into train / test datasets:\n",
    "df_imdb_train_neg = df_imdb_neg[:400]\n",
    "df_imdb_test_neg = df_imdb_neg[400:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon - Divide to positive and negative data set and extract from there the train / test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into positive / negative datasets:\n",
    "df_amazon_pos = df_amazon[df_amazon['label'] == 1].reset_index(drop=True)   # positive emotion = 1\n",
    "df_amazon_neg = df_amazon[df_amazon['label'] == 0].reset_index(drop=True)   # Negative emotion = 0\n",
    "\n",
    "# For positive - train / test datasets:\n",
    "df_amazon_train_pos = df_amazon_pos[:400]\n",
    "df_amazon_test_pos = df_amazon_pos[400:500]\n",
    "\n",
    "# For the negative dataset - divide into train / test datasets:\n",
    "df_amazon_train_neg = df_amazon_neg[:400]\n",
    "df_amazon_test_neg = df_amazon_neg[400:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat - train_df & test_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.concat([df_amazon_train_pos, df_amazon_train_neg, df_yelp_train_pos, df_yelp_train_neg,\n",
    "                                   df_imdb_train_pos, df_imdb_train_neg], ignore_index=True)\n",
    "\n",
    "#self check\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.concat([df_amazon_test_pos, df_amazon_test_neg, df_yelp_test_pos, df_yelp_test_neg,\n",
    "                                   df_yelp_test_pos, df_yelp_test_neg], ignore_index=True)\n",
    "\n",
    "#self check\n",
    "len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2(h)\n",
    "df_train_ngram = df_train.copy()\n",
    "df_test_ngram = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words model.\n",
    "\n",
    "Extract features and then represent each review using bag of words model, i.e., every word in the review becomes its own element in a feature vector. \n",
    "In order to do this, first, make one pass through all the reviews in the training set (Explain why we can’t\n",
    "use testing set at this point) and build a dictionary of unique words. \n",
    "Then, make another pass through the review in both the training set and testing set and count up the occurrences of\n",
    "each word in your dictionary. \n",
    "The i-th element of a review’s feature vector is the number of occurrences of the ith dictionary word in the review. \n",
    "Implement the bag of words model and report feature vectors of any two reviews in the training set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dictionary with all unique words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with all unique words and their count:\n",
    "word_freq = defaultdict(int)\n",
    "for row in range(0,len(df_train)):\n",
    "    for i in df_train['sentence'][row].split():\n",
    "        word_freq[i] += 1\n",
    "\n",
    "#print(word_freq, len(word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vector_func(sentence):\n",
    "    feature_vector = {x:0 for x in word_freq} # Put 0-s instead of counting every unique word\n",
    "    word_list = sentence.split()\n",
    "    counter = 0\n",
    "    for word in word_freq:\n",
    "        if word in word_list:\n",
    "            feature_vector[word] = counter + 1\n",
    "        else:\n",
    "            feature_vector[word] = counter\n",
    "    return(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1505"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the function to all rows in df_train to get all feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['feature_vector_dict'] = df_train['sentence'].apply(lambda x: feature_vector_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go far mani place never seen restaur serf 1 eg...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'go': 1, 'far': 1, 'mani': 1, 'place': 1, 'ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delici absolut back</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>over like place lot</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go': 0, 'far': 0, 'mani': 0, 'place': 1, 'ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fantast servic</td>\n",
       "      <td>0</td>\n",
       "      <td>{'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im az time new spot</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0  go far mani place never seen restaur serf 1 eg...      0   \n",
       "1                                delici absolut back      1   \n",
       "2                                over like place lot      1   \n",
       "3                                     fantast servic      0   \n",
       "4                                im az time new spot      1   \n",
       "\n",
       "                                 feature_vector_dict  \n",
       "0  {'go': 1, 'far': 1, 'mani': 1, 'place': 1, 'ne...  \n",
       "1  {'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...  \n",
       "2  {'go': 0, 'far': 0, 'mani': 0, 'place': 1, 'ne...  \n",
       "3  {'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...  \n",
       "4  {'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the function to all rows in the same manner for df_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['feature_vector_dict'] = df_test['sentence'].apply(lambda x: feature_vector_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>close hous lowkey nonfanc afford price good food</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bad day low toler rude custom servic peopl job...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nt mani word say place doe everyth pretti well</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go': 0, 'far': 0, 'mani': 1, 'place': 1, 'ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thing wa nt crazi wa guacamol nt like puré</td>\n",
       "      <td>0</td>\n",
       "      <td>{'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>want first say server wa great perfect servic</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0   close hous lowkey nonfanc afford price good food      1   \n",
       "1  bad day low toler rude custom servic peopl job...      0   \n",
       "2     nt mani word say place doe everyth pretti well      1   \n",
       "3         thing wa nt crazi wa guacamol nt like puré      0   \n",
       "4      want first say server wa great perfect servic      1   \n",
       "\n",
       "                                 feature_vector_dict  \n",
       "0  {'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...  \n",
       "1  {'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...  \n",
       "2  {'go': 0, 'far': 0, 'mani': 1, 'place': 1, 'ne...  \n",
       "3  {'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...  \n",
       "4  {'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare any two feature vectors of any two reviews in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'go': 1, 'far': 1, 'mani': 1, 'place': 1, 'ne...\n",
       "1    {'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...\n",
       "Name: feature_vector_dict, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['feature_vector_dict'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(e) - Pick your postprocessing strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We chose to use the 4th method - standardize the data by subtracting the mean and dividing by the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. \n",
    "\n",
    "However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. \n",
    "\n",
    "So, even if you have outliers in your data, they will not be affected by standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that converts the dict \"feature_vector\" into an array:\n",
    "def dict_to_array(vector):\n",
    "    data = list(vector.values())\n",
    "    an_array = np.array(data)\n",
    "    return(an_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = list(df_train['feature_vector'][1].values())\n",
    "# an_array = np.array(data)\n",
    "# print(an_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['feature_vector_array'] = df_train['feature_vector_dict'].apply(lambda x: dict_to_array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #self check - ignore\n",
    "# arr_2 = df_train['feature_vector_array'][2]\n",
    "# for i in arr_2:\n",
    "#     if i == 1:\n",
    "#         print(i)\n",
    "\n",
    "# df_train['sentence'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that gets \"df_train['array_vector'][i]\", and returns the standardized array:\n",
    "\n",
    "import itertools\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def standard(row):\n",
    "    standardized_array = scaler.fit_transform(np.array(row).reshape(-1,1))\n",
    "    return(standardized_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['standard_array'] = df_train['feature_vector_array'].apply(lambda x: standard(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.71304022],\n",
       "       [10.71304022],\n",
       "       [10.71304022],\n",
       "       ...,\n",
       "       [-0.09334418],\n",
       "       [-0.09334418],\n",
       "       [-0.09334418]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#self check - ignore\n",
    "df_train['standard_array'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict</th>\n",
       "      <th>feature_vector_array</th>\n",
       "      <th>standard_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go far mani place never seen restaur serf 1 eg...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'go': 1, 'far': 1, 'mani': 1, 'place': 1, 'ne...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "      <td>[[10.71304022064842], [10.71304022064842], [10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delici absolut back</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[[-0.04469157509144717], [-0.04469157509144717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>over like place lot</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go': 0, 'far': 0, 'mani': 0, 'place': 1, 'ne...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.051622573291966733], [-0.0516225732919667...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fantast servic</td>\n",
       "      <td>0</td>\n",
       "      <td>{'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.036478377010972195], [-0.0364783770109721...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im az time new spot</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[-0.05773502691896259], [-0.05773502691896259...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0  go far mani place never seen restaur serf 1 eg...      0   \n",
       "1                                delici absolut back      1   \n",
       "2                                over like place lot      1   \n",
       "3                                     fantast servic      0   \n",
       "4                                im az time new spot      1   \n",
       "\n",
       "                                 feature_vector_dict  \\\n",
       "0  {'go': 1, 'far': 1, 'mani': 1, 'place': 1, 'ne...   \n",
       "1  {'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...   \n",
       "2  {'go': 0, 'far': 0, 'mani': 0, 'place': 1, 'ne...   \n",
       "3  {'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...   \n",
       "4  {'go': 0, 'far': 0, 'mani': 0, 'place': 0, 'ne...   \n",
       "\n",
       "                                feature_vector_array  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "2  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                      standard_array  \n",
       "0  [[10.71304022064842], [10.71304022064842], [10...  \n",
       "1  [[-0.04469157509144717], [-0.04469157509144717...  \n",
       "2  [[-0.051622573291966733], [-0.0516225732919667...  \n",
       "3  [[-0.036478377010972195], [-0.0364783770109721...  \n",
       "4  [[-0.05773502691896259], [-0.05773502691896259...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['feature_vector_array'] = df_test['feature_vector_dict'].apply(lambda x: dict_to_array(x))\n",
    "df_test['standard_array'] = df_test['feature_vector_array'].apply(lambda x: standard(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in range(0, len(df_test['feature_vector_array'])):\n",
    "#     for i in df_test['feature_vector_array'][row]:\n",
    "#         if i==1:\n",
    "#             print(row, i, df_test['sentence'][row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(f) - Sentiment Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a naive Bayes model on the training set and test on the testing set. \n",
    "\n",
    "Report the classification accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the data comfortable to use:\n",
    "def array_of_arrays(row):\n",
    "    list_standard = []\n",
    "    for i in range(0, len(row)):\n",
    "        list_standard.append((row[i][0]))\n",
    "    return (np.array(list_standard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['standard_array_clean'] = df_train['standard_array'].apply(lambda x: array_of_arrays(x))\n",
    "df_test['standard_array_clean'] = df_test['standard_array'].apply(lambda x: array_of_arrays(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = df_train[\"standard_array_clean\"]\n",
    "test_X = df_test[\"standard_array_clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.DataFrame(train_X.to_list()).to_numpy()\n",
    "test_X = pd.DataFrame(test_X.to_list()).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Check that there is no \"Nan\" values:\n",
    "\n",
    "for array in test_X:\n",
    "    array_sum = np.sum(array)\n",
    "    array_has_nan = np.isnan(array_sum)\n",
    "print(array_has_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = df_train['label']\n",
    "test_Y = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = train_X.shape[0] #number of rows\n",
    "d = train_X.shape[1] #number of unique words = features in feature vector\n",
    "K = 2 #number of classes - label 1 or label 0\n",
    "\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    X_k = train_X[train_Y == k]\n",
    "    phis[k] = X_k.shape[0] / float(n)\n",
    "    psis[k] = np.mean(X_k, axis=0) #build a function with the mean for label 0 and mean for label 0 as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(psis.shape, phis.shape, train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement this in numpy\n",
    "def nb_predictions(x, psis, phis):\n",
    "    \"\"\"This returns class assignments and scores under the NB model.\n",
    "    \n",
    "    We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n",
    "    \"\"\"\n",
    "    # adjust shapes\n",
    "    n , d = x.shape\n",
    "    x = np.reshape(x, (1,n,d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14) #understand\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape(K,1)\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes on the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train, logpyx = nb_predictions(train_X, psis, phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(predicted_train), len(logpyx), predicted_train.shape, logpyx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6541666666666667"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_train == train_Y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test, logpyx_test = nb_predictions(test_X, psis, phis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6766666666666666"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_test == test_Y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = test_Y\n",
    "predictions = predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x211480801c0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get actual and create heatmap \n",
    "confusion_matrix = pd.crosstab(actual, predictions , rownames=['Actual'], colnames=['Predicted'], normalize=True)\n",
    "plt.subplots(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(g): Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way to put the data - it gets the same results\n",
    "# train_x_ridge = df_train['feature_vector_array']\n",
    "# test_x_ridge = df_test['feature_vector_array']\n",
    "\n",
    "# train_x_ridge = pd.DataFrame(train_x_ridge.to_list()).to_numpy()\n",
    "# test_x_ridge = pd.DataFrame(test_x_ridge.to_list()).to_numpy()\n",
    "\n",
    "# train_y_ridge = train_Y\n",
    "# test_y_ridge = test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_list = list(word_freq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_feat_vect = pd.DataFrame(df_train['feature_vector_array'].to_list())\n",
    "train_X_feat_vect.columns = list(word_freq.keys())\n",
    "test_X_feat_vect = pd.DataFrame(df_test['feature_vector_array'].to_list())\n",
    "test_X_feat_vect.columns = list(word_freq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = LogisticRegressionCV(cv=10, penalty='l2', solver='liblinear').fit(train_X_feat_vect, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ridge.predict(test_X_feat_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.685"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ridge.score(test_X_feat_vect, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most and least important words for the ridge resgression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge.coef_\n",
    "coeffs_ridge = list(np.argsort(model_ridge.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delici\n",
      "great\n",
      "good\n",
      "fantast\n",
      "amaz\n",
      "pretti\n",
      "staff\n",
      "fresh\n",
      "vega\n",
      "waitress\n"
     ]
    }
   ],
   "source": [
    "# most important words:\n",
    "most_important = coeffs_ridge[0][::-1]\n",
    "for i in range(0,10):\n",
    "    print(unique_words_list[most_important[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "minut\n",
      "one\n",
      "nt\n",
      "wait\n",
      "worst\n",
      "rude\n",
      "suck\n",
      "3\n",
      "expect\n"
     ]
    }
   ],
   "source": [
    "least_important = coeffs_ridge[0]\n",
    "for i in range(0,10):\n",
    "    print(unique_words_list[least_important[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ophir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "model_lasso = LogisticRegressionCV(cv=10, penalty='l1', solver='liblinear').fit(train_X_feat_vect, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lasso.predict(test_X_feat_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lasso.score(test_X_feat_vect, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a dictionary of n-gram (n=2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all two consecutive words in all rows in the dataframe: \n",
    "list_consecutive = []\n",
    "for row in range(0, len(df_train['sentence'])):\n",
    "    sentence = df_train['sentence'][row]\n",
    "    word_list = df_train['sentence'][row].split()\n",
    "    i=0\n",
    "    while i < len(word_list)-1:\n",
    "        if (word_list[i] + ' ' + word_list[i+1]) not in list_consecutive:\n",
    "            list_consecutive.append(word_list[i]+ ' ' + word_list[i+1])\n",
    "            i+=1\n",
    "        else:\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(list_consecutive) #self-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of consecutive into a dictionary\n",
    "ngram_dict = dict.fromkeys(list_consecutive, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat 2(d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that return the two consecutive for every row in the dataframe:\n",
    "def consecutive(sentence):\n",
    "    list_cons_per_row = []\n",
    "    word_list = sentence.split()\n",
    "    i=0\n",
    "    while i < len(word_list)-1:\n",
    "        if (word_list[i] + ' ' + word_list[i+1]) not in list_cons_per_row:\n",
    "            list_cons_per_row.append(word_list[i]+ ' ' + word_list[i+1])\n",
    "            i+=1\n",
    "        else:\n",
    "            i+=1\n",
    "    return(list_cons_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_list_3 = consecutive(df_train['sentence'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in word_list_3:\n",
    "#     print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vector_func_ngram(sentence):\n",
    "    feature_vector = {x:0 for x in ngram_dict}\n",
    "    word_list = consecutive(sentence)\n",
    "    counter = 0\n",
    "    for word in word_list:\n",
    "        if word in ngram_dict:\n",
    "            feature_vector[word] = counter + 1\n",
    "        else:\n",
    "            feature_vector[word] = counter\n",
    "    return(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ngram['feature_vector_dict_ngram'] = df_train_ngram['sentence'].apply(lambda x: feature_vector_func_ngram(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go far mani place never seen restaur serf 1 eg...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'go far': 1, 'far mani': 1, 'mani place': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delici absolut back</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go far': 0, 'far mani': 0, 'mani place': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>over like place lot</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go far': 0, 'far mani': 0, 'mani place': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fantast servic</td>\n",
       "      <td>0</td>\n",
       "      <td>{'go far': 0, 'far mani': 0, 'mani place': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im az time new spot</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go far': 0, 'far mani': 0, 'mani place': 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0  go far mani place never seen restaur serf 1 eg...      0   \n",
       "1                                delici absolut back      1   \n",
       "2                                over like place lot      1   \n",
       "3                                     fantast servic      0   \n",
       "4                                im az time new spot      1   \n",
       "\n",
       "                           feature_vector_dict_ngram  \n",
       "0  {'go far': 1, 'far mani': 1, 'mani place': 1, ...  \n",
       "1  {'go far': 0, 'far mani': 0, 'mani place': 0, ...  \n",
       "2  {'go far': 0, 'far mani': 0, 'mani place': 0, ...  \n",
       "3  {'go far': 0, 'far mani': 0, 'mani place': 0, ...  \n",
       "4  {'go far': 0, 'far mani': 0, 'mani place': 0, ...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ngram.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the function to all rows in the same manner for df_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_ngram['feature_vector_dict_ngram'] = df_test_ngram['sentence'].apply(lambda x: feature_vector_func_ngram(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>close hous lowkey nonfanc afford price good food</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go far': 0, 'far mani': 0, 'mani place': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bad day low toler rude custom servic peopl job...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'go far': 0, 'far mani': 0, 'mani place': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nt mani word say place doe everyth pretti well</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go far': 0, 'far mani': 0, 'mani place': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thing wa nt crazi wa guacamol nt like puré</td>\n",
       "      <td>0</td>\n",
       "      <td>{'go far': 0, 'far mani': 0, 'mani place': 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>want first say server wa great perfect servic</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go far': 0, 'far mani': 0, 'mani place': 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0   close hous lowkey nonfanc afford price good food      1   \n",
       "1  bad day low toler rude custom servic peopl job...      0   \n",
       "2     nt mani word say place doe everyth pretti well      1   \n",
       "3         thing wa nt crazi wa guacamol nt like puré      0   \n",
       "4      want first say server wa great perfect servic      1   \n",
       "\n",
       "                           feature_vector_dict_ngram  \n",
       "0  {'go far': 0, 'far mani': 0, 'mani place': 0, ...  \n",
       "1  {'go far': 0, 'far mani': 0, 'mani place': 0, ...  \n",
       "2  {'go far': 0, 'far mani': 0, 'mani place': 0, ...  \n",
       "3  {'go far': 0, 'far mani': 0, 'mani place': 0, ...  \n",
       "4  {'go far': 0, 'far mani': 0, 'mani place': 0, ...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_ngram.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare any two feature vectors of any two reviews in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'go far': 1, 'far mani': 1, 'mani place': 1, ...\n",
       "1    {'go far': 0, 'far mani': 0, 'mani place': 0, ...\n",
       "Name: feature_vector_dict_ngram, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ngram['feature_vector_dict_ngram'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat 2(e) - Postprocessing Strategy with ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ngram['feature_vector_array_ngram'] = df_train_ngram['feature_vector_dict_ngram'].apply(lambda x: dict_to_array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict_ngram</th>\n",
       "      <th>feature_vector_array_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go far mani place never seen restaur serf 1 eg...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'go far': 1, 'far mani': 1, 'mani place': 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delici absolut back</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go far': 0, 'far mani': 0, 'mani place': 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>over like place lot</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go far': 0, 'far mani': 0, 'mani place': 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fantast servic</td>\n",
       "      <td>0</td>\n",
       "      <td>{'go far': 0, 'far mani': 0, 'mani place': 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im az time new spot</td>\n",
       "      <td>1</td>\n",
       "      <td>{'go far': 0, 'far mani': 0, 'mani place': 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0  go far mani place never seen restaur serf 1 eg...      0   \n",
       "1                                delici absolut back      1   \n",
       "2                                over like place lot      1   \n",
       "3                                     fantast servic      0   \n",
       "4                                im az time new spot      1   \n",
       "\n",
       "                           feature_vector_dict_ngram  \\\n",
       "0  {'go far': 1, 'far mani': 1, 'mani place': 1, ...   \n",
       "1  {'go far': 0, 'far mani': 0, 'mani place': 0, ...   \n",
       "2  {'go far': 0, 'far mani': 0, 'mani place': 0, ...   \n",
       "3  {'go far': 0, 'far mani': 0, 'mani place': 0, ...   \n",
       "4  {'go far': 0, 'far mani': 0, 'mani place': 0, ...   \n",
       "\n",
       "                          feature_vector_array_ngram  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ngram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ngram['standard_array_ngram'] = df_train_ngram['feature_vector_array_ngram'].apply(lambda x: standard(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_ngram['feature_vector_array_ngram'] = df_test_ngram['feature_vector_dict_ngram'].apply(lambda x: dict_to_array(x))\n",
    "df_test_ngram['standard_array_ngram'] = df_test_ngram['feature_vector_array_ngram'].apply(lambda x: standard(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3798"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test_ngram['standard_array_ngram'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reapeat 2(f) - Sentiment Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a naive Bayes model on the training set and test on the testing set. \n",
    "\n",
    "Report the classification accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ngram['standard_array_clean_ngram'] = df_train_ngram['standard_array_ngram'].apply(lambda x: array_of_arrays(x))\n",
    "df_test_ngram['standard_array_clean_ngram'] = df_test_ngram['standard_array_ngram'].apply(lambda x: array_of_arrays(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3798"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test_ngram['standard_array_clean_ngram'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_ngram = df_train_ngram[\"standard_array_clean_ngram\"]\n",
    "test_X_ngram = df_test_ngram[\"standard_array_clean_ngram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Check that there is no \"Nan\" values:\n",
    "for array in test_X_ngram:\n",
    "    array_sum = np.sum(array)\n",
    "    array_has_nan = np.isnan(array_sum)\n",
    "print(array_has_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#False = no \"Nan\" values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_ngram = pd.DataFrame(train_X_ngram.to_list())\n",
    "# train_X_ngram = train_X_ngram.loc[:, :3753]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Check again - no \"Nan\" values\n",
    "for array in test_X_ngram:\n",
    "    array_sum = np.sum(array)\n",
    "    array_has_nan = np.isnan(array_sum)\n",
    "print(array_has_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_ngram = train_X_ngram.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3796"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_ngram.shape[1] #works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3797</th>\n",
       "      <th>3798</th>\n",
       "      <th>3799</th>\n",
       "      <th>3800</th>\n",
       "      <th>3801</th>\n",
       "      <th>3802</th>\n",
       "      <th>3803</th>\n",
       "      <th>3804</th>\n",
       "      <th>3805</th>\n",
       "      <th>3806</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 3807 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0   -0.036307 -0.036307 -0.036307 -0.036307 -0.036307 -0.036307 -0.036307   \n",
       "1   -0.056306 -0.056306 -0.056306 -0.056306 -0.056306 -0.056306 -0.056306   \n",
       "2   -0.045956 -0.045956 -0.045956 -0.045956 -0.045956 -0.045956 -0.045956   \n",
       "3   -0.022942 -0.022942 -0.022942 -0.022942 -0.022942 -0.022942 -0.022942   \n",
       "4   -0.028109 -0.028109 -0.028109 -0.028109 -0.028109 -0.028109 -0.028109   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "595  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "596  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "597  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "598 -0.036317 -0.036317 -0.036317 -0.036317 -0.036317 -0.036317 -0.036317   \n",
       "599  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         7         8         9     ...      3797      3798      3799  \\\n",
       "0   -0.036307 -0.036307 -0.036307  ... -0.036307       NaN       NaN   \n",
       "1   -0.056306 -0.056306 -0.056306  ...       NaN       NaN       NaN   \n",
       "2   -0.045956 -0.045956 -0.045956  ...       NaN       NaN       NaN   \n",
       "3   -0.022942 -0.022942 -0.022942  ... -0.022942 -0.022942 -0.022942   \n",
       "4   -0.028109 -0.028109 -0.028109  ... -0.028109 -0.028109 -0.028109   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "595  0.000000  0.000000  0.000000  ...  0.000000  0.000000       NaN   \n",
       "596  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "597  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "598 -0.036317 -0.036317 -0.036317  ...       NaN       NaN       NaN   \n",
       "599  0.000000  0.000000  0.000000  ...  0.000000  0.000000       NaN   \n",
       "\n",
       "         3800      3801  3802  3803  3804  3805  3806  \n",
       "0         NaN       NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "1         NaN       NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2         NaN       NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "3   -0.022942 -0.022942   NaN   NaN   NaN   NaN   NaN  \n",
       "4         NaN       NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "..        ...       ...   ...   ...   ...   ...   ...  \n",
       "595       NaN       NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "596  0.000000  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "597       NaN       NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "598       NaN       NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "599       NaN       NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[600 rows x 3807 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_ngram = pd.DataFrame(test_X_ngram.to_list())\n",
    "test_X_ngram #I see there is redundant data - drop all columns after 3796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping redundant columns\n",
    "test_X_ngram = test_X_ngram.loc[:, :3795]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3786</th>\n",
       "      <th>3787</th>\n",
       "      <th>3788</th>\n",
       "      <th>3789</th>\n",
       "      <th>3790</th>\n",
       "      <th>3791</th>\n",
       "      <th>3792</th>\n",
       "      <th>3793</th>\n",
       "      <th>3794</th>\n",
       "      <th>3795</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "      <td>-0.036307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "      <td>-0.056306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "      <td>-0.045956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.022942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "      <td>-0.028109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "      <td>-0.036317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 3796 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0   -0.036307 -0.036307 -0.036307 -0.036307 -0.036307 -0.036307 -0.036307   \n",
       "1   -0.056306 -0.056306 -0.056306 -0.056306 -0.056306 -0.056306 -0.056306   \n",
       "2   -0.045956 -0.045956 -0.045956 -0.045956 -0.045956 -0.045956 -0.045956   \n",
       "3   -0.022942 -0.022942 -0.022942 -0.022942 -0.022942 -0.022942 -0.022942   \n",
       "4   -0.028109 -0.028109 -0.028109 -0.028109 -0.028109 -0.028109 -0.028109   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "595  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "596  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "597  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "598 -0.036317 -0.036317 -0.036317 -0.036317 -0.036317 -0.036317 -0.036317   \n",
       "599  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         7         8         9     ...      3786      3787      3788  \\\n",
       "0   -0.036307 -0.036307 -0.036307  ... -0.036307 -0.036307 -0.036307   \n",
       "1   -0.056306 -0.056306 -0.056306  ... -0.056306 -0.056306 -0.056306   \n",
       "2   -0.045956 -0.045956 -0.045956  ... -0.045956 -0.045956 -0.045956   \n",
       "3   -0.022942 -0.022942 -0.022942  ... -0.022942 -0.022942 -0.022942   \n",
       "4   -0.028109 -0.028109 -0.028109  ... -0.028109 -0.028109 -0.028109   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "595  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "596  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "597  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "598 -0.036317 -0.036317 -0.036317  ... -0.036317 -0.036317 -0.036317   \n",
       "599  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "         3789      3790      3791      3792      3793      3794      3795  \n",
       "0   -0.036307 -0.036307 -0.036307 -0.036307 -0.036307 -0.036307 -0.036307  \n",
       "1   -0.056306 -0.056306 -0.056306 -0.056306 -0.056306 -0.056306 -0.056306  \n",
       "2   -0.045956 -0.045956 -0.045956 -0.045956 -0.045956 -0.045956 -0.045956  \n",
       "3   -0.022942 -0.022942 -0.022942 -0.022942 -0.022942 -0.022942 -0.022942  \n",
       "4   -0.028109 -0.028109 -0.028109 -0.028109 -0.028109 -0.028109 -0.028109  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "595  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "596  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "597  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "598 -0.036317 -0.036317 -0.036317 -0.036317 -0.036317 -0.036317 -0.036317  \n",
       "599  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[600 rows x 3796 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check again:\n",
    "test_X_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#check there is no \"Nan\" values\n",
    "for array in test_X_ngram:\n",
    "    array_sum = np.sum(array)\n",
    "    array_has_nan = np.isnan(array_sum)\n",
    "print(array_has_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#False - we can continue:\n",
    "test_X_ngram = test_X_ngram.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 3796)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_ngram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y_ngram = df_train['label']\n",
    "test_Y_ngram = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = train_X_ngram.shape[0] #number of rows\n",
    "d = train_X_ngram.shape[1] #number of unique words = features in feature vector\n",
    "K = 2 #number of classes - label 1 or label 0\n",
    "\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    X_k_ngram = train_X_ngram[train_Y_ngram == k]\n",
    "    phis[k] = X_k_ngram.shape[0] / float(n)\n",
    "    psis[k] = np.mean(X_k_ngram, axis=0) #build a function with the mean for label 0 and mean for label 0 as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3796) (2,) (2400, 3796)\n"
     ]
    }
   ],
   "source": [
    "print(psis.shape, phis.shape, train_X_ngram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement this in numpy\n",
    "def nb_predictions(x, psis, phis):\n",
    "    \"\"\"This returns class assignments and scores under the NB model.\n",
    "    \n",
    "    We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n",
    "    \"\"\"\n",
    "    # adjust shapes\n",
    "    n , d = x.shape\n",
    "    x = np.reshape(x, (1,n,d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14) #understand\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape(K,1)\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes on the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train_ngram, logpyx_ngram = nb_predictions(train_X_ngram, psis, phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in predicted_train_ngram:\n",
    "#     if i != 0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7266666666666667"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_train_ngram == train_Y_ngram).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_ngram, logpyx_test = nb_predictions(test_X_ngram, psis, phis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5766666666666667"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_test_ngram == test_Y_ngram).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ngram = test_Y_ngram\n",
    "predictions_ngram = predicted_test_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_ngram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_ngram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2113b5369d0>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get actual and create heatmap \n",
    "confusion_matrix_ngram = pd.crosstab(actual_ngram, predictions_ngram , rownames=['Actual'], colnames=['Predicted'], normalize=True)\n",
    "plt.subplots(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix_ngram, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat 2(g): Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_feat_vect_ngram = pd.DataFrame(df_train_ngram['feature_vector_array_ngram'].to_list())\n",
    "train_X_feat_vect_ngram.columns = list(ngram_dict.keys())\n",
    "test_X_feat_vect_ngram = pd.DataFrame(df_test_ngram['feature_vector_array_ngram'].to_list())\n",
    "test_X_feat_vect_ngram = test_X_feat_vect_ngram.loc[:, :3795]\n",
    "test_X_feat_vect_ngram.columns = list(ngram_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge_ngram = LogisticRegressionCV(cv=10, penalty='l2', solver='liblinear').fit(train_X_feat_vect_ngram, train_Y_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ridge_ngram.predict(test_X_feat_vect_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6033333333333334"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ridge_ngram.score(test_X_feat_vect_ngram, test_Y_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most and least important words for the ridge resgression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge_ngram.coef_\n",
    "coeffs_ridge_ngram = list(np.argsort(model_ridge_ngram.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ngram = list(ngram_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolut back\n",
      "3 time\n",
      "food wa\n",
      "az time\n",
      "chicken dish\n",
      "green heart\n",
      "worth price\n",
      "feel like\n",
      "burger friend\n",
      "good though\n"
     ]
    }
   ],
   "source": [
    "# most important words:\n",
    "most_important_ngram = coeffs_ridge_ngram[0][::-1]\n",
    "for i in range(0,10):\n",
    "    print(list_ngram[most_important[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come back\n",
      "best chee\n",
      "wa incr\n",
      "ice tea\n",
      "time food\n",
      "ravoli chicken\n",
      "duck wa\n",
      "mussel cook\n",
      "probabl wo\n",
      "wa greatest\n"
     ]
    }
   ],
   "source": [
    "least_important_ngram = coeffs_ridge_ngram[0]\n",
    "for i in range(0,10):\n",
    "    print(list_ngram[least_important[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso_ngram = LogisticRegressionCV(cv=10, penalty='l1', solver='liblinear').fit(train_X_feat_vect_ngram, train_Y_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lasso_ngram.predict(test_X_feat_vect_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5266666666666666"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lasso_ngram.score(test_X_feat_vect_ngram, test_Y_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Comparison and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above results, compare the performances of naive Bayes, logistic regression, naive Bayes with 2-grams, and logistic regression\n",
    "with 2-grams. Which method performs best in the prediction task and why? What do you\n",
    "learn about the language that people use in online reviews (e.g., expressions that will make\n",
    "the posts positive/negative)? Hint: Inspect the weights learned from logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general - part (1) is better than part (2).\n",
    "\n",
    "In n-gram model:\n",
    "    lasso accuracy < ridge accuracy because Lasso penalizes for many features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE - QUESTIONS FOR ME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso & Ridge - do twice the importance of words or once? weird results for lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso.coef_\n",
    "coeffs_lasso = list(np.argsort(model_lasso.coef_))\n",
    "# print(coeffs_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important words:\n",
    "most_important_lasso = coeffs_lasso[0][::-1]\n",
    "for i in range(0,10):\n",
    "    print(unique_words_list[most_important[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_important = coeff[0]\n",
    "for i in range(0,10):\n",
    "    print(unique_words_list[least_important[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  *** Why is all of that not giving me good scores? How is it differentthan the ridge part? *** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing alpha vs. not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression with 10-fold cross validation to chose ¸\n",
    "# The Ridge() function has an alpha argument ( λ , but with a different name!) that is used to tune the model\n",
    "# Alpha values ranging from very big to very small, essentially covering the full \n",
    "# range of scenarios from the null model containing only the intercept, to the least squares fit\n",
    "\n",
    "alphas = 10**np.linspace(10,-2,100)*0.5 \n",
    "\n",
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(train_x_ridge, train_y_ridge)\n",
    "ridgecv.alpha_\n",
    "\n",
    "ridge = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "ridge.fit(train_x_ridge, train_y_ridge)\n",
    "mean_squared_error(test_y_ridge, ridge.predict(test_x_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression with 10-fold cross validation to chose ¸\n",
    "# The Ridge() function has an alpha argument ( λ , but with a different name!) that is used to tune the model\n",
    "# Alpha values ranging from very big to very small, essentially covering the full \n",
    "# range of scenarios from the null model containing only the intercept, to the least squares fit\n",
    "\n",
    "alphas = 10**np.linspace(10,-2,100)*0.5 \n",
    "\n",
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(train_X_feat_vect, train_Y)\n",
    "ridgecv.alpha_\n",
    "\n",
    "ridge = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "ridge.fit(train_X_feat_vect, train_Y)\n",
    "mean_squared_error(test_Y, ridge.predict(test_X_feat_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge.predict(test_x_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.score(train_x_ridge, train_y_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgereg = Ridge(alpha=1.0,normalize=True)\n",
    "ridgereg.fit(train_x_ridge, train_y_ridge)\n",
    "y_predict = ridgereg.predict(train_x_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is this different than the other ridge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RidgeClassifier().fit(train_x_ridge, train_y_ridge)\n",
    "clf.score(train_x_ridge, train_y_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RidgeClassifier().fit(test_x_ridge, test_y_ridge)       # clf = Ridge(alpha=1.0)\n",
    "clf.score(test_x_ridge, test_y_ridge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
