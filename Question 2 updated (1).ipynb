{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ophir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ophir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ophir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.neighbors\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#lemmatization libraries \n",
    "import nltk.stem\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# punctuation + stop words libraries:\n",
    "from nltk.corpus import stopwords\n",
    "import re, string, timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1(a) Parsing the txt files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1(a): Parsing Yelp, IMDB, and Amazon database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = pd.read_csv(\"yelp_labelled.txt\", sep=\"\\t\", header=None, names=[\"sentence\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many values are there? How many positive and negative emotions (0/1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-0a2494051e90>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df_imdb = pd.read_csv(\"imdb_labelled.txt\", sep=\" \\t\", header=None, names=[\"sentence\", \"label\"])\n"
     ]
    }
   ],
   "source": [
    "df_imdb = pd.read_csv(\"imdb_labelled.txt\", sep=\" \\t\", header=None, names=[\"sentence\", \"label\"])\n",
    "#Notice: IMDB required another space in order to parse the 1000 lines because it was badly formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many values are there? How many positive and negative emotions (0/1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imdb.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Amazon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon = pd.read_csv(\"amazon_cells_labelled.txt\", sep=\"\\t\", header=None, names=[\"sentence\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many values are there? How many positive and negative emotions (0/1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results - Amazon, IMDB and Yelp are balanced (50% negative 50% positive)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(b): Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick your preprocessing strategy. Since these sentences are online reviews, they may contain significant amounts of noise and garbage. You may or may not want to do one or all of\n",
    "the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Lowercase all of the words.\n",
    "\n",
    "• Lemmatization of all the words (i.e., convert every word to its root so that all of “running,”\n",
    "“run,” and “runs” are converted to “run” and and all of “good,” “well,” “better,” and “best”\n",
    "are converted to “good”; this is easily done using nltk.stem).\n",
    "\n",
    "• Strip punctuation.\n",
    "\n",
    "• Strip the stop words, e.g., “the”, “and”, “or”.\n",
    "\n",
    "• Something else? Tell us about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Lowercase all databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[\"sentence\"] = df_yelp[\"sentence\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb[\"sentence\"] = df_imdb[\"sentence\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon[\"sentence\"] = df_amazon[\"sentence\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Punctuation Strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that removes punctuation:\n",
    "def remove_punctuation(sentence):\n",
    "    sentence = re.sub(r'[^\\w\\s]','',sentence)\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp[\"sentence\"] = df_yelp['sentence'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb[\"sentence\"] = df_imdb['sentence'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon[\"sentence\"] = df_amazon['sentence'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Strip the stop words, e.g., “the”, “and”, “or”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['sentence'] = df_yelp['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure there are no stop words in the database:\n",
    "# df_yelp[\"sentence\"].str.contains(\"the\", na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb['sentence'] = df_imdb['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon['sentence'] = df_amazon['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Lemmatiation of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that returns a lemmatized sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    list2 = nltk.word_tokenize(text)\n",
    "    lemmatized_sentence = ' '.join([wnl.lemmatize(words) for words in list2])\n",
    "    return(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply \"lemmatization\" on the \"sentence\" column on all datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['sentence'] = df_yelp['sentence'].apply(lambda x: lemmatize_text(x))\n",
    "df_imdb['sentence'] = df_imdb['sentence'].apply(lambda x: lemmatize_text(x))\n",
    "df_amazon['sentence'] = df_amazon['sentence'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 (extra): stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to add stemming because it is helps us to achieve the root forms (synonyms) of inflected words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    w_list = text.split()\n",
    "    stem_list = []\n",
    "    for word in w_list:\n",
    "        stemmed_word = porter.stem(word)\n",
    "        stem_list.append(stemmed_word)\n",
    "    stemmed_sentence = ' '.join(stem_list)\n",
    "    return(stemmed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply \"stemming\" on the \"sentence\" column on all datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['sentence'] = df_yelp['sentence'].apply(lambda x: stemming(x))\n",
    "df_imdb['sentence'] = df_yelp['sentence'].apply(lambda x: stemming(x))\n",
    "df_amazon['sentence'] = df_yelp['sentence'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2(C): Split training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each file use the first 400 instances for each label as the training set and the remaining 100 instances as testing set.\n",
    "\n",
    "In total, there are 2400 reviews for training and 600 reviews for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yelp - Divide to positive and negative data set and extract from there the train / test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into positive / negative datasets:\n",
    "df_yelp_pos = df_yelp[df_yelp['label'] == 1].reset_index(drop=True)   # positive emotion = 1\n",
    "df_yelp_neg = df_yelp[df_yelp['label'] == 0].reset_index(drop=True)   # Negative emotion = 0\n",
    "\n",
    "# For the positive dataset - divide into train / test datasets:\n",
    "df_yelp_train_pos = df_yelp_pos[:400]\n",
    "df_yelp_test_pos = df_yelp_pos[400:500]\n",
    "\n",
    "# For the negative dataset - divide into train / test datasets:\n",
    "df_yelp_train_neg = df_yelp_neg[:400]\n",
    "df_yelp_test_neg = df_yelp_neg[400:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB - Divide to positive and negative data set and extract from there the train / test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into positive / negative datasets:\n",
    "df_imdb_pos = df_imdb[df_imdb['label'] == 1].reset_index(drop=True)   # positive emotion = 1\n",
    "df_imdb_neg = df_imdb[df_imdb['label'] == 0].reset_index(drop=True)   # Negative emotion = 0\n",
    "\n",
    "# For positive - train / test datasets:\n",
    "df_imdb_train_pos = df_imdb_pos[:400]\n",
    "df_imdb_test_pos = df_imdb_pos[400:500]\n",
    "\n",
    "# For the negative dataset - divide into train / test datasets:\n",
    "df_imdb_train_neg = df_imdb_neg[:400]\n",
    "df_imdb_test_neg = df_imdb_neg[400:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon - Divide to positive and negative data set and extract from there the train / test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into positive / negative datasets:\n",
    "df_amazon_pos = df_amazon[df_amazon['label'] == 1].reset_index(drop=True)   # positive emotion = 1\n",
    "df_amazon_neg = df_amazon[df_amazon['label'] == 0].reset_index(drop=True)   # Negative emotion = 0\n",
    "\n",
    "# For positive - train / test datasets:\n",
    "df_amazon_train_pos = df_amazon_pos[:400]\n",
    "df_amazon_test_pos = df_amazon_pos[400:500]\n",
    "\n",
    "# For the negative dataset - divide into train / test datasets:\n",
    "df_amazon_train_neg = df_amazon_neg[:400]\n",
    "df_amazon_test_neg = df_amazon_neg[400:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat - train_df & test_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.concat([df_amazon_train_neg, df_amazon_train_pos, df_yelp_train_neg, df_yelp_train_pos,\n",
    "                                   df_imdb_train_neg, df_imdb_train_pos], ignore_index=True)\n",
    "\n",
    "#self check\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.concat([df_amazon_test_neg, df_amazon_test_pos, df_yelp_test_neg, df_yelp_test_pos,\n",
    "                                   df_yelp_test_neg, df_yelp_test_pos], ignore_index=True)\n",
    "\n",
    "#self check\n",
    "len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words model.\n",
    "\n",
    "Extract features and then represent each review using bag of words model, i.e., every word in the review becomes its own element in a feature vector. \n",
    "In order to do this, first, make one pass through all the reviews in the training set (Explain why we can’t\n",
    "use testing set at this point) and build a dictionary of unique words. \n",
    "Then, make another pass through the review in both the training set and testing set and count up the occurrences of\n",
    "each word in your dictionary. \n",
    "The i-th element of a review’s feature vector is the number of occurrences of the ith dictionary word in the review. \n",
    "Implement the bag of words model and report feature vectors of any two reviews in the training set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dictionary with all unique words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with all unique words and their count:\n",
    "word_freq = defaultdict(int)\n",
    "for row in range(0,len(df_train)):\n",
    "    for i in df_train['sentence'][row].split():\n",
    "        word_freq[i] += 1\n",
    "\n",
    "#print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vector_func(sentence):\n",
    "    feature_vector = {x:0 for x in word_freq} # Put 0-s instead of counting every unique word\n",
    "    word_list = sentence.split()\n",
    "    counter = 0\n",
    "    for word in word_freq:\n",
    "        if word in word_list:\n",
    "            feature_vector[word] = counter + 1\n",
    "        else:\n",
    "            feature_vector[word] = counter\n",
    "    return(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the function to all rows in df_train to get all feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['feature_vector_dict'] = df_train['sentence'].apply(lambda x: feature_vector_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wow love place</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 1, 'love': 1, 'place': 1, 'stop': 0, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stop late may bank holiday rick steve recommen...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 0, 'love': 1, 'place': 0, 'stop': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>get angri want damn pho</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>honeslti didnt tast fresh</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fri great</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0                                     wow love place      0   \n",
       "1  stop late may bank holiday rick steve recommen...      0   \n",
       "2                            get angri want damn pho      0   \n",
       "3                          honeslti didnt tast fresh      0   \n",
       "4                                          fri great      0   \n",
       "\n",
       "                                 feature_vector_dict  \n",
       "0  {'wow': 1, 'love': 1, 'place': 1, 'stop': 0, '...  \n",
       "1  {'wow': 0, 'love': 1, 'place': 0, 'stop': 1, '...  \n",
       "2  {'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...  \n",
       "3  {'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...  \n",
       "4  {'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the function to all rows in the same manner for df_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['feature_vector_dict'] = df_test['sentence'].apply(lambda x: feature_vector_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>servic fair best</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>favor stay away dish</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>poor servic</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>one tabl thought food averag worth wait</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best servic food ever maria server good friend...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0                                   servic fair best      0   \n",
       "1                               favor stay away dish      0   \n",
       "2                                        poor servic      0   \n",
       "3            one tabl thought food averag worth wait      0   \n",
       "4  best servic food ever maria server good friend...      0   \n",
       "\n",
       "                                 feature_vector_dict  \n",
       "0  {'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...  \n",
       "1  {'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...  \n",
       "2  {'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...  \n",
       "3  {'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...  \n",
       "4  {'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare any two feature vectors of any two reviews in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'wow': 1, 'love': 1, 'place': 1, 'stop': 0, '...\n",
       "1    {'wow': 0, 'love': 1, 'place': 0, 'stop': 1, '...\n",
       "Name: feature_vector_dict, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['feature_vector_dict'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(e) - Pick your postprocessing strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We chose to use the 4th method - standardize the data by subtracting the mean and dividing by the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. \n",
    "\n",
    "However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. \n",
    "\n",
    "So, even if you have outliers in your data, they will not be affected by standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that converts the dict \"feature_vector\" into an array:\n",
    "def dict_to_array(vector):\n",
    "    data = list(vector.values())\n",
    "    an_array = np.array(data)\n",
    "    return(an_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = list(df_train['feature_vector'][1].values())\n",
    "# an_array = np.array(data)\n",
    "# print(an_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['feature_vector_array'] = df_train['feature_vector_dict'].apply(lambda x: dict_to_array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #self check - ignore\n",
    "# arr_2 = df_train['feature_vector_array'][2]\n",
    "# for i in arr_2:\n",
    "#     if i == 1:\n",
    "#         print(i)\n",
    "\n",
    "# df_train['sentence'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that gets \"df_train['array_vector'][i]\", and returns the standardized array:\n",
    "def standard(dataset):\n",
    "    standardized_array = (dataset - np.average(dataset)) / (np.std(dataset))\n",
    "    return(standardized_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['standard_array'] = df_train['feature_vector_array'].apply(lambda x: standard(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.61268081, 22.61268081, 22.61268081, ..., -0.04422297,\n",
       "       -0.04422297, -0.04422297])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#self check - ignore\n",
    "df_train['standard_array'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self check \n",
    "# for i in df_train['standard_array'][1]:\n",
    "#     if i != -0.04606127153588057:\n",
    "#         print(i, df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_vector_dict</th>\n",
       "      <th>feature_vector_array</th>\n",
       "      <th>standard_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wow love place</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 1, 'love': 1, 'place': 1, 'stop': 0, '...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[22.612680808195506, 22.612680808195506, 22.61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stop late may bank holiday rick steve recommen...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 0, 'love': 1, 'place': 0, 'stop': 1, '...</td>\n",
       "      <td>[0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.07674667651449762, 13.029880190461377, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>get angri want damn pho</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-0.057128868112378745, -0.057128868112378745,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>honeslti didnt tast fresh</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.051080944423879705, -0.051080944423879705,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fri great</td>\n",
       "      <td>0</td>\n",
       "      <td>{'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.03609614378422167, -0.03609614378422167, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0                                     wow love place      0   \n",
       "1  stop late may bank holiday rick steve recommen...      0   \n",
       "2                            get angri want damn pho      0   \n",
       "3                          honeslti didnt tast fresh      0   \n",
       "4                                          fri great      0   \n",
       "\n",
       "                                 feature_vector_dict  \\\n",
       "0  {'wow': 1, 'love': 1, 'place': 1, 'stop': 0, '...   \n",
       "1  {'wow': 0, 'love': 1, 'place': 0, 'stop': 1, '...   \n",
       "2  {'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...   \n",
       "3  {'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...   \n",
       "4  {'wow': 0, 'love': 0, 'place': 0, 'stop': 0, '...   \n",
       "\n",
       "                                feature_vector_array  \\\n",
       "0  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                      standard_array  \n",
       "0  [22.612680808195506, 22.612680808195506, 22.61...  \n",
       "1  [-0.07674667651449762, 13.029880190461377, -0....  \n",
       "2  [-0.057128868112378745, -0.057128868112378745,...  \n",
       "3  [-0.051080944423879705, -0.051080944423879705,...  \n",
       "4  [-0.03609614378422167, -0.03609614378422167, -...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['feature_vector_array'] = df_test['feature_vector_dict'].apply(lambda x: dict_to_array(x))\n",
    "df_test['standard_array'] = df_test['feature_vector_array'].apply(lambda x: standard(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1537"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test['standard_array'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(f) - Sentiment Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a naive Bayes model on the training set and test on the testing set. \n",
    "\n",
    "Report the classification accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = df_train[\"standard_array\"]\n",
    "test_X = df_test[\"standard_array\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.DataFrame(train_X.to_list())\n",
    "test_X = pd.DataFrame(test_X.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.to_numpy()\n",
    "test_X = test_X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = df_train['label']\n",
    "test_Y = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22.61268081, 22.61268081, 22.61268081, ..., -0.04422297,\n",
       "        -0.04422297, -0.04422297],\n",
       "       [-0.07674668, 13.02988019, -0.07674668, ..., -0.07674668,\n",
       "        -0.07674668, -0.07674668],\n",
       "       [-0.05712887, -0.05712887, -0.05712887, ..., -0.05712887,\n",
       "        -0.05712887, -0.05712887],\n",
       "       ...,\n",
       "       [-0.06260197, -0.06260197, -0.06260197, ..., -0.06260197,\n",
       "        -0.06260197, -0.06260197],\n",
       "       [-0.04422297, -0.04422297, 22.61268081, ..., -0.04422297,\n",
       "        -0.04422297, -0.04422297],\n",
       "       [-0.04422297, -0.04422297, -0.04422297, ..., -0.04422297,\n",
       "        -0.04422297, -0.04422297]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#self check\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = train_X.shape[0] #number of rows\n",
    "d = train_X.shape[1] #number of unique words = features in feature vector\n",
    "K = 2 #number of classes - label 1 or label 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    X_k = train_X[train_Y == k]\n",
    "    phis[k] = X_k.shape[0] / float(n)\n",
    "    psis[k] = np.mean(X_k, axis=0) #build a function with the mean for label 0 and mean for label 0 as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(psis.shape, phis.shape, train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can implement this in numpy\n",
    "def nb_predictions(x, psis, phis):\n",
    "    \"\"\"This returns class assignments and scores under the NB model.\n",
    "    \n",
    "    We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n",
    "    \"\"\"\n",
    "    # adjust shapes\n",
    "    n , d = x.shape\n",
    "    x = np.reshape(x, (1,n,d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14) #understand\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape(K,1)\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "\n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes on the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted, logpyx = nb_predictions(train_X, psis, phis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6408333333333334"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted == train_Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted #len(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes on the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test, logpyx_test = nb_predictions(test_X, psis, phis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_test == test_Y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = test_Y\n",
    "predictions = predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x237d2b6a7f0>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get actual and create heatmap \n",
    "confusion_matrix = pd.crosstab(actual, predictions , rownames=['Actual'], colnames=['Predicted'], normalize=True)\n",
    "plt.subplots(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2(g): Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
